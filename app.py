st.plotly_chart(fig_daily, use_container_width=True)
        
        # An√°lise de taxa de convers√£o di√°ria
        col_conv1, col_conv2 = st.columns([0.7, 0.3])
        
        with col_conv1:
            st.markdown("#### üéØ Taxa de Convers√£o Di√°ria")
            
            fig_conversion = go.Figure()
            
            fig_conversion.add_trace(go.Scatter(
                x=daily_data["dia"],
                y=daily_data["Taxa_Conversao"],
                mode="lines+markers",
                name="Taxa Di√°ria",
                line=dict(color="#f59e0b", width=2),
                marker=dict(size=4)
            ))
            
            if "MA7_Taxa" in daily_data.columns:
                fig_conversion.add_trace(go.Scatter(
                    x=daily_data["dia"],
                    y=daily_data["MA7_Taxa"],
                    mode="lines",
                    name="M√©dia M√≥vel 7 dias",
                    line=dict(color="#ef4444", width=3, dash="dash")
                ))
            
            if show_benchmarks:
                fig_conversion.add_hline(
                    y=70, 
                    line_dash="dot", 
                    line_color="red",
                    annotation_text="Meta: 70%"
                )
                fig_conversion.add_hline(
                    y=50, 
                    line_dash="dot", 
                    line_color="orange",
                    annotation_text="M√≠nimo: 50%"
                )
            
            fig_conversion.update_layout(
                height=400,
                hovermode="x unified",
                xaxis_title="Data",
                yaxis_title="Taxa (%)",
                yaxis=dict(range=[0, 100])
            )
            
            st.plotly_chart(fig_conversion, use_container_width=True)
        
        with col_conv2:
            st.markdown("#### üìä Distribui√ß√£o Semanal")
            
            # An√°lise por dia da semana
            weekly_pattern = daily_data.groupby("Dia_Semana").agg({
                "Atendimentos": "mean",
                "Laudos": "mean",
                "Taxa_Conversao": "mean"
            }).round(1)
            
            # Reordenar dias da semana
            day_order = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
            day_names_pt = ["Segunda", "Ter√ßa", "Quarta", "Quinta", "Sexta", "S√°bado", "Domingo"]
            
            weekly_pattern = weekly_pattern.reindex([day for day in day_order if day in weekly_pattern.index])
            weekly_pattern.index = [day_names_pt[day_order.index(day)] for day in weekly_pattern.index]
            
            fig_weekly = px.bar(
                weekly_pattern.reset_index(),
                x="Dia_Semana",
                y=["Atendimentos", "Laudos"],
                title="M√©dia por Dia da Semana",
                barmode="group"
            )
            
            fig_weekly.update_layout(height=300, showlegend=True)
            st.plotly_chart(fig_weekly, use_container_width=True)
            
            # Estat√≠sticas semanais
            st.markdown("**üìà Estat√≠sticas:**")
            melhor_dia = weekly_pattern["Taxa_Conversao"].idxmax()
            pior_dia = weekly_pattern["Taxa_Conversao"].idxmin()
            
            st.write(f"üèÜ **Melhor dia:** {melhor_dia} ({weekly_pattern.loc[melhor_dia, 'Taxa_Conversao']:.1f}%)")
            st.write(f"üìâ **Pior dia:** {pior_dia} ({weekly_pattern.loc[pior_dia, 'Taxa_Conversao']:.1f}%)")
        
        # An√°lises avan√ßadas
        st.markdown("#### üîç An√°lises Avan√ßadas")
        
        analysis_col1, analysis_col2, analysis_col3 = st.columns(3)
        
        with analysis_col1:
            st.markdown("**üìä Distribui√ß√£o de Volumes**")
            
            # Histograma de atendimentos di√°rios
            fig_hist = px.histogram(
                daily_data,
                x="Atendimentos",
                nbins=20,
                title="Distribui√ß√£o de Atendimentos Di√°rios"
            )
            fig_hist.update_layout(height=300)
            st.plotly_chart(fig_hist, use_container_width=True)
        
        with analysis_col2:
            st.markdown("**üìà An√°lise de Tend√™ncia**")
            
            if len(daily_data) >= 30:
                # √öltimos 30 dias
                recent_data = daily_data.tail(30)
                
                # C√°lculo de tend√™ncia
                from scipy import stats
                x_vals = np.arange(len(recent_data))
                slope_atend, _, r_atend, _, _ = stats.linregress(x_vals, recent_data["Atendimentos"])
                slope_laudos, _, r_laudos, _, _ = stats.linregress(x_vals, recent_data["Laudos"])
                
                trend_atend = "‚ÜóÔ∏è Crescente" if slope_atend > 0 else "‚ÜòÔ∏è Decrescente"
                trend_laudos = "‚ÜóÔ∏è Crescente" if slope_laudos > 0 else "‚ÜòÔ∏è Decrescente"
                
                st.metric("Tend√™ncia Atendimentos", trend_atend, f"R¬≤: {r_atend**2:.3f}")
                st.metric("Tend√™ncia Laudos", trend_laudos, f"R¬≤: {r_laudos**2:.3f}")
                
                # Previs√£o simples para pr√≥ximos 7 dias
                if abs(r_atend) > 0.5:  # Apenas se correla√ß√£o for razo√°vel
                    next_7_days = pd.date_range(start=daily_data["dia"].max() + pd.Timedelta(days=1), periods=7)
                    x_future = np.arange(len(recent_data), len(recent_data) + 7)
                    pred_atend = slope_atend * x_future[-1] + recent_data["Atendimentos"].iloc[0]
                    
                    st.write(f"üìÆ **Previs√£o (7d):** {pred_atend:.0f} atendimentos")
        
        with analysis_col3:
            st.markdown("**üö® Alertas Operacionais**")
            
            # Detectar anomalias baseadas em desvio padr√£o
            if len(daily_data) >= 14:
                # Usar √∫ltimas 2 semanas como baseline
                baseline = daily_data.tail(14)
                
                mean_atend = baseline["Atendimentos"].mean()
                std_atend = baseline["Atendimentos"].std()
                
                mean_laudos = baseline["Laudos"].mean()
                std_laudos = baseline["Laudos"].std()
                
                # √öltimo dia
                last_atend = ultimo_registro["Atendimentos"]
                last_laudos = ultimo_registro["Laudos"]
                
                # Verificar anomalias (>2 desvios padr√£o)
                alerts = []
                
                if abs(last_atend - mean_atend) > 2 * std_atend:
                    direction = "acima" if last_atend > mean_atend else "abaixo"
                    alerts.append(f"üî¥ Atendimentos {direction} do normal")
                
                if abs(last_laudos - mean_laudos) > 2 * std_laudos:
                    direction = "acima" if last_laudos > mean_laudos else "abaixo"
                    alerts.append(f"üî¥ Laudos {direction} do normal")
                
                if ultimo_registro["Taxa_Conversao"] < 30:
                    alerts.append("üî¥ Taxa de convers√£o cr√≠tica")
                
                if not alerts:
                    st.success("‚úÖ Opera√ß√£o normal")
                else:
                    for alert in alerts:
                        st.warning(alert)
                
                # M√©tricas de baseline
                st.metric("Baseline Atendimentos", f"{mean_atend:.0f} ¬± {std_atend:.0f}")
                st.metric("Baseline Laudos", f"{mean_laudos:.0f} ¬± {std_laudos:.0f}")
        
        # Tabela de dados recentes
        st.markdown("#### üìã Dados Recentes (√öltimos 30 dias)")
        
        recent_table = daily_data.tail(30).copy()
        recent_table["dia"] = recent_table["dia"].dt.strftime("%d/%m/%Y")
        recent_table["Dia_Semana"] = recent_table["Dia_Semana"].map({
            "Monday": "Segunda", "Tuesday": "Ter√ßa", "Wednesday": "Quarta",
            "Thursday": "Quinta", "Friday": "Sexta", "Saturday": "S√°bado", "Sunday": "Domingo"
        })
        
        # Formata√ß√£o da tabela
        display_columns = ["dia", "Dia_Semana", "Atendimentos", "Laudos", "Taxa_Conversao"]
        if "MA7_Atendimentos" in recent_table.columns:
            display_columns.extend(["MA7_Atendimentos", "MA7_Laudos"])
        
        # Aplicar formata√ß√£o
        for col in ["Atendimentos", "Laudos"]:
            if col in recent_table.columns:
                recent_table[col] = recent_table[col].apply(lambda x: f"{int(x):,}".replace(",", "."))
        
        for col in ["Taxa_Conversao", "MA7_Atendimentos", "MA7_Laudos"]:
            if col in recent_table.columns:
                recent_table[col] = recent_table[col].apply(lambda x: f"{x:.1f}" if pd.notna(x) else "‚Äî")
        
        available_columns = [col for col in display_columns if col in recent_table.columns]
        st.dataframe(recent_table[available_columns].sort_values("dia", ascending=False), use_container_width=True, height=400)
        
        # Download de dados
        csv_daily = daily_data.to_csv(index=False).encode("utf-8")
        st.download_button(
            label="üì• Download Dados Di√°rios (CSV)",
            data=csv_daily,
            file_name=f"analise_diaria_pci_sc_{datetime.now().strftime('%Y%m%d')}.csv",
            mime="text/csv"
        )

# ============ ABA 6: DADOS BRUTOS ============ 
with tab6:
    st.subheader("üìã Explora√ß√£o e Qualidade dos Dados")
    
    # Resumo geral dos datasets
    st.markdown("#### üìä Resumo dos Datasets Carregados")
    
    data_summary = []
    for name, df in dataframes.items():
        if df is not None and not df.empty:
            # Informa√ß√µes b√°sicas
            periodo_info = "Sem dados temporais"
            if 'anom√™s' in df.columns and not df['anom√™s'].isna().all():
                periodo_info = f"{df['anom√™s'].min()} a {df['anom√™s'].max()}"
            elif 'dia' in df.columns and not df['dia'].isna().all():
                min_date = df['dia'].min().strftime("%d/%m/%Y") if pd.notna(df['dia'].min()) else "N/A"
                max_date = df['dia'].max().strftime("%d/%m/%Y") if pd.notna(df['dia'].max()) else "N/A"
                periodo_info = f"{min_date} a {max_date}"
            
            # C√°lculo de qualidade
            total_cells = len(df) * len(df.columns)
            null_cells = df.isnull().sum().sum()
            quality_score = ((total_cells - null_cells) / total_cells) * 100 if total_cells > 0 else 0
            
            quality_status = "üü¢ Excelente" if quality_score >= 95 else "üü° Boa" if quality_score >= 85 else "üü† Regular" if quality_score >= 70 else "üî¥ Ruim"
            
            data_summary.append({
                "Dataset": name.replace("_", " ").title(),
                "Registros": f"{len(df):,}".replace(",", "."),
                "Colunas": len(df.columns),
                "Per√≠odo": periodo_info,
                "Qualidade": quality_status,
                "Tamanho (MB)": round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2),
                "Status": "‚úÖ Ativo" if name in filtered_dataframes and not filtered_dataframes[name].empty else "‚ö†Ô∏è Filtrado"
            })
    
    if data_summary:
        summary_df = pd.DataFrame(data_summary)
        st.dataframe(summary_df, use_container_width=True)
        
        # M√©tricas consolidadas
        total_registros = sum(int(row["Registros"].replace(".", "")) for row in data_summary)
        total_tamanho = sum(row["Tamanho (MB)"] for row in data_summary)
        datasets_ativos = sum(1 for row in data_summary if row["Status"] == "‚úÖ Ativo")
        
        summary_col1, summary_col2, summary_col3, summary_col4 = st.columns(4)
        with summary_col1:
            st.metric("Total de Registros", f"{total_registros:,}".replace(",", "."))
        with summary_col2:
            st.metric("Datasets Carregados", len(data_summary))
        with summary_col3:
            st.metric("Datasets Ativos", datasets_ativos)
        with summary_col4:
            st.metric("Tamanho Total", f"{total_tamanho:.1f} MB")
    
    # Explora√ß√£o detalhada
    st.markdown("#### üîç Explora√ß√£o Detalhada por Dataset")
    
    available_datasets = [name for name, df in dataframes.items() if df is not None and not df.empty]
    if available_datasets:
        selected_dataset = st.selectbox(
            "Selecione o dataset para explorar:",
            available_datasets,
            format_func=lambda x: x.replace("_", " ").title()
        )
        
        if selected_dataset:
            df_selected = dataframes[selected_dataset]
            df_filtered = filtered_dataframes.get(selected_dataset, df_selected)
            
            # Informa√ß√µes b√°sicas
            info_col1, info_col2, info_col3, info_col4 = st.columns(4)
            with info_col1:
                st.metric("Registros Totais", f"{len(df_selected):,}".replace(",", "."))
            with info_col2:
                st.metric("Registros Filtrados", f"{len(df_filtered):,}".replace(",", "."))
            with info_col3:
                st.metric("Colunas", len(df_selected.columns))
            with info_col4:
                null_percentage = (df_selected.isnull().sum().sum() / (len(df_selected) * len(df_selected.columns))) * 100
                st.metric("% Valores Nulos", f"{null_percentage:.1f}%")
            
            # An√°lise de qualidade detalhada
            with st.expander("üîç An√°lise de Qualidade dos Dados", expanded=False):
                quality_analysis = []
                
                for col in df_selected.columns:
                    col_data = df_selected[col]
                    dtype = str(col_data.dtype)
                    
                    # Estat√≠sticas b√°sicas
                    null_count = col_data.isnull().sum()
                    null_pct = (null_count / len(df_selected)) * 100
                    unique_count = col_data.nunique()
                    unique_pct = (unique_count / len(df_selected)) * 100
                    
                    # Detec√ß√£o de tipo de dados
                    if col_data.dtype in ['int64', 'float64']:
                        data_type = "üî¢ Num√©rico"
                        sample_values = f"Min: {col_data.min()}, Max: {col_data.max()}"
                    elif col_data.dtype == 'datetime64[ns]' or 'data' in col.lower():
                        data_type = "üìÖ Data/Hora"
                        sample_values = f"De: {col_data.min()}, At√©: {col_data.max()}" if not col_data.isna().all() else "Datas inv√°lidas"
                    else:
                        data_type = "üìù Texto"
                        top_values = col_data.value_counts().head(3)
                        sample_values = ", ".join([f"{k}: {v}" for k, v in top_values.items()]) if not top_values.empty else "Sem dados"
                    
                    # Score de qualidade da coluna
                    quality_score = 100 - null_pct
                    if unique_pct < 1:  # Muito poucos valores √∫nicos
                        quality_score *= 0.8
                    
                    quality_level = "üü¢ Excelente" if quality_score >= 95 else "üü° Boa" if quality_score >= 85 else "üü† Regular" if quality_score >= 70 else "üî¥ Ruim"
                    
                    quality_analysis.append({
                        "Coluna": col,
                        "Tipo": data_type,
                        "Valores √önicos": f"{unique_count:,}".replace(",", "."),
                        "% √önicos": f"{unique_pct:.1f}%",
                        "Nulos": f"{null_count:,}".replace(",", "."),
                        "% Nulos": f"{null_pct:.1f}%",
                        "Qualidade": quality_level,
                        "Amostra": sample_values[:50] + "..." if len(str(sample_values)) > 50 else sample_values
                    })
                
                quality_df = pd.DataFrame(quality_analysis)
                st.dataframe(quality_df, use_container_width=True, height=400)
            
            # Controles de visualiza√ß√£o
            st.markdown("**üéõÔ∏è Controles de Visualiza√ß√£o**")
            viz_col1, viz_col2, viz_col3, viz_col4 = st.columns(4)
            
            with viz_col1:
                max_rows = st.number_input(
                    "M√°ximo de linhas:",
                    min_value=10,
                    max_value=5000,
                    value=min(500, len(df_filtered)),
                    step=50
                )
            
            with viz_col2:
                # Filtro temporal se dispon√≠vel
                temporal_filter = None
                if 'anom√™s' in df_filtered.columns:
                    available_months = sorted(df_filtered['anom√™s'].dropna().unique(), reverse=True)
                    temporal_filter = st.multiselect(
                        "Filtrar por per√≠odo:",
                        available_months,
                        default=available_months[:6] if len(available_months) > 6 else available_months
                    )
            
            with viz_col3:
                # Sele√ß√£o de colunas
                all_columns = list(df_filtered.columns)
                selected_columns = st.multiselect(
                    "Colunas a exibir:",
                    all_columns,
                    default=all_columns[:10] if len(all_columns) > 10 else all_columns
                )
            
            with viz_col4:
                # Ordena√ß√£o
                sort_column = st.selectbox(
                    "Ordenar por:",
                    ["Nenhum"] + list(df_filtered.columns)
                )
            
            # Aplica√ß√£o dos controles
            df_display = df_filtered.copy()
            
            # Filtro temporal
            if temporal_filter and 'anom√™s' in df_display.columns:
                df_display = df_display[df_display['anom√™s'].isin(temporal_filter)]
            
            # Sele√ß√£o de colunas
            if selected_columns:
                df_display = df_display[selected_columns]
            
            # Ordena√ß√£o
            if sort_column != "Nenhum" and sort_column in df_display.columns:
                df_display = df_display.sort_values(sort_column, ascending=False)
            
            # Limita√ß√£o de linhas
            df_display = df_display.head(max_rows)
            
            # Estat√≠sticas descritivas para colunas num√©ricas
            numeric_cols = df_display.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                st.markdown("**üìà Estat√≠sticas Descritivas (Colunas Num√©ricas):**")
                stats = df_display[numeric_cols].describe().round(2)
                st.dataframe(stats, use_container_width=True)
            
            # Exibi√ß√£o dos dados
            st.markdown(f"**üìã Dados Selecionados ({len(df_display):,} de {len(df_selected):,} registros):**".replace(",", "."))
            st.dataframe(df_display, use_container_width=True, height=400)
            
            # Downloads
            download_col1, download_col2, download_col3 = st.columns(3)
            
            with download_col1:
                csv_filtered = df_display.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="üì• Download Dados Filtrados (CSV)",
                    data=csv_filtered,
                    file_name=f"{selected_dataset}_filtrado_{datetime.now().strftime('%Y%m%d')}.csv",
                    mime="text/csv"
                )
            
            with download_col2:
                csv_complete = df_selected.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="üì• Download Dataset Completo (CSV)",
                    data=csv_complete,
                    file_name=f"{selected_dataset}_completo_{datetime.now().strftime('%Y%m%d')}.csv",
                    mime="text/csv"
                )
            
            with download_col3:
                # Relat√≥rio de qualidade
                quality_report = f"""# Relat√≥rio de Qualidade - {selected_dataset}
                
## Informa√ß√µes Gerais
- **Total de Registros:** {len(df_selected):,}
- **Total de Colunas:** {len(df_selected.columns)}
- **Per√≠odo:** {periodo_info}
- **Tamanho:** {df_selected.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB

## Qualidade dos Dados
- **Valores Nulos:** {df_selected.isnull().sum().sum():,} ({null_percentage:.1f}%)
- **Colunas com Nulos:** {(df_selected.isnull().sum() > 0).sum()}
- **Score de Qualidade:** {100 - null_percentage:.1f}%

## Colunas Analisadas
{chr(10).join([f"- **{row['Coluna']}**: {row['Tipo']}, {row['% Nulos']} nulos" for _, row in quality_df.iterrows()])}

---
*Relat√≥rio gerado em {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}*
"""
                
                st.download_button(
                    label="üìÑ Download Relat√≥rio de Qualidade (MD)",
                    data=quality_report.encode('utf-8'),
                    file_name=f"relatorio_qualidade_{selected_dataset}_{datetime.now().strftime('%Y%m%d')}.md",
                    mime="text/markdown"
                )

# ============ ABA 7: RELAT√ìRIOS ============
with tab7:
    st.subheader("üìë Relat√≥rios Executivos e Exporta√ß√µes")
    
    # Sele√ß√£o do tipo de relat√≥rio
    report_col1, report_col2 = st.columns([0.7, 0.3])
    
    with report_col1:
        report_type = st.selectbox(
            "üéØ Tipo de Relat√≥rio:",
            [
                "Relat√≥rio Executivo Completo",
                "Relat√≥rio de Produ√ß√£o",
                "Relat√≥rio de Pend√™ncias",
                "Relat√≥rio de Performance",
                "Relat√≥rio de Tend√™ncias",
                "Relat√≥rio Operacional Di√°rio"
            ]
        )
    
    with report_col2:
        report_format = st.selectbox(
            "üìÑ Formato de Exporta√ß√£o:",
            ["Markdown", "PDF", "HTML", "JSON"]
        )
    
    def generate_executive_report() -> str:
        """Gera relat√≥rio executivo completo"""
        timestamp = datetime.now().strftime("%d/%m/%Y %H:%M:%S")
        
        # C√°lculo de insights adicionais
        insights = []
        
        if crescimento_laudos:
            if crescimento_laudos > 10:
                insights.append(f"üìà **Crescimento Forte**: Laudos cresceram {format_number(crescimento_laudos,1)}% no per√≠odo")
            elif crescimento_laudos < -10:
                insights.append(f"üìâ **Alerta de Queda**: Laudos decresceram {format_number(abs(crescimento_laudos),1)}% no per√≠odo")
        
        if taxa_conversao:
            if taxa_conversao > 80:
                insights.append(f"üéØ **Alta Efici√™ncia**: Taxa de convers√£o de {format_number(taxa_conversao,1)}% acima da meta")
            elif taxa_conversao < 50:
                insights.append(f"‚ö†Ô∏è **Baixa Efici√™ncia**: Taxa de convers√£o de {format_number(taxa_conversao,1)}% abaixo do aceit√°vel")
        
        # Recomenda√ß√µes baseadas em dados
        recommendations = []
        
        if backlog_meses and backlog_meses > 6:
            recommendations.append("üî¥ **URGENTE**: Implementar plano de redu√ß√£o de backlog com metas semanais")
        
        if taxa_conversao and taxa_conversao < 60:
            recommendations.append("üü° **MELHORIA**: Revisar processos de convers√£o de atendimentos em laudos")
        
        if total_pend_laudos > total_pend_exames * 2:
            recommendations.append("üü° **PROCESSO**: Investigar gargalos na finaliza√ß√£o de laudos")
        
        report = f"""# üìä RELAT√ìRIO EXECUTIVO PCI/SC

**Data de Gera√ß√£o:** {timestamp}  
**Per√≠odo de An√°lise:** {period_filter}  
**Filtros Aplicados:** {len([f for f in dimensional_filters.values() if f])} filtros ativos

---

## üéØ RESUMO EXECUTIVO

### Indicadores Principais
| M√©trica | Valor | Status |
|---------|-------|--------|
| **Atendimentos Totais** | {format_number(total_atendimentos)} | {("üü¢" if crescimento_atendimentos and crescimento_atendimentos > 0 else "üî¥")} |
| **Laudos Emitidos** | {format_number(total_laudos)} | {("üü¢" if crescimento_laudos and crescimento_laudos > 0 else "üî¥")} |
| **Taxa de Convers√£o** | {format_number(taxa_conversao, 1) if taxa_conversao else 'N/A'}% | {("üü¢" if taxa_conversao and taxa_conversao >= 70 else "üü°" if taxa_conversao and taxa_conversao >= 50 else "üî¥")} |
| **Produtividade Mensal** | {format_number(media_mensal_laudos, 1) if media_mensal_laudos else 'N/A'} laudos | - |

---

## ‚è∞ SITUA√á√ÉO DE PEND√äNCIAS

### Backlog Atual
- **Laudos Pendentes:** {format_number(total_pend_laudos)} casos
- **Exames Pendentes:** {format_number(total_pend_exames)} casos
- **Backlog Estimado:** {format_number(backlog_meses, 1) if backlog_meses else 'N/A'} meses
- **Aging M√©dio:** {format_number(aging_laudos.get("media_dias") or aging_exames.get("media_dias"), 0) if (aging_laudos.get("media_dias") or aging_exames.get("media_dias")) else 'N/A'} dias

### Casos Cr√≠ticos (>90 dias)
- **Laudos Cr√≠ticos:** {aging_laudos.get("criticos", 0)} casos
- **Exames Cr√≠ticos:** {aging_exames.get("criticos", 0)} casos

---

## üìà AN√ÅLISE DE PERFORMANCE

### Tend√™ncias Identificadas
{chr(10).join(insights) if insights else "- Sem tend√™ncias significativas identificadas no per√≠odo"}

### Crescimento Per√≠odo
- **Atendimentos:** {format_number(crescimento_atendimentos, 1) if crescimento_atendimentos else 'N/A'}%
- **Laudos:** {format_number(crescimento_laudos, 1) if crescimento_laudos else 'N/A'}%

---

## üö® ALERTAS E RECOMENDA√á√ïES

### Recomenda√ß√µes Priorit√°rias
{chr(10).join(recommendations) if recommendations else "‚úÖ **Situa√ß√£o Normal**: Todos os indicadores dentro dos par√¢metros esperados"}

### Plano de A√ß√£o Sugerido
1. **Curto Prazo (30 dias):**
   - Monitorar diariamente casos com aging > 90 dias
   - Implementar reuni√µes semanais de acompanhamento de backlog

2. **M√©dio Prazo (90 dias):**
   - Otimizar processos de convers√£o de atendimentos
   - Estabelecer metas de produtividade por unidade

3. **Longo Prazo (180 dias):**
   - Implementar sistema de alertas autom√°ticos
   - Desenvolver pain√©is de monitoramento em tempo real

---

## üìä DADOS UTILIZADOS

### Datasets Processados
{chr(10).join([f"- **{name.replace('_', ' ').title()}**: {len(df):,} registros".replace(",", ".") for name, df in dataframes.items() if df is not None and not df.empty])}

### Per√≠odo de Dados
- **Dados Mais Antigos:** {min([df['anom√™s'].min() for df in dataframes.values() if df is not None and 'anom√™s' in df.columns and not df['anom√™s'].isna().all()], default='N/A')}
- **Dados Mais Recentes:** {max([df['anom√™s'].max() for df in dataframes.values() if df is not None and 'anom√™s' in df.columns and not df['anom√™s'].isna().all()], default='N/A')}

---

## üìù METODOLOGIA

### C√°lculos Realizados
- **Taxa de Convers√£o:** (Total Laudos / Total Atendimentos) √ó 100
- **Crescimento:** Compara√ß√£o entre primeiros e √∫ltimos 3 meses do per√≠odo
- **Backlog:** Total Pend√™ncias / Produtividade Mensal M√©dia
- **Aging:** Dias corridos desde a data de solicita√ß√£o

### Crit√©rios de Alerta
- üü¢ **Normal:** Taxa convers√£o > 70%, Backlog < 3 meses
- üü° **Aten√ß√£o:** Taxa convers√£o 50-70%, Backlog 3-6 meses  
- üî¥ **Cr√≠tico:** Taxa convers√£o < 50%, Backlog > 6 meses

---

*Relat√≥rio gerado automaticamente pelo Dashboard PCI/SC v3.0*  
*Sistema de Monitoramento Executivo - Desenvolvido para otimiza√ß√£o operacional*
"""
        
        return report.strip()
    
    def generate_production_report() -> str:
        """Gera relat√≥rio espec√≠fico de produ√ß√£o"""
        timestamp = datetime.now().strftime("%d/%m/%Y %H:%M:%S")
        
        # An√°lise de top performers
        top_unidades = []
        if df_laudos_todos is not None and "unidade" in df_laudos_todos.columns:
            unidades_ranking = df_laudos_todos.groupby("unidade")["quantidade"].sum().sort_values(ascending=False).head(10)
            top_unidades = [f"- **{unidade}**: {format_number(total)} laudos" for unidade, total in unidades_ranking.items()]
        
        # An√°lise temporal
        monthly_trend = "Est√°vel"
        if crescimento_laudos:
            if crescimento_laudos > 5:
                monthly_trend = f"Crescimento de {format_number(crescimento_laudos,1)}%"
            elif crescimento_laudos < -5:
                monthly_trend = f"Queda de {format_number(abs(crescimento_laudos),1)}%"
        
        return f"""# üìà RELAT√ìRIO DE PRODU√á√ÉO PCI/SC

**Data:** {timestamp}  
**Per√≠odo:** {period_filter}

## üéØ RESUMO DE PRODU√á√ÉO

### Volumes Totais
- **Atendimentos Realizados:** {format_number(total_atendimentos)}
- **Laudos Emitidos:** {format_number(total_laudos)}
- **M√©dia Mensal de Laudos:** {format_number(media_mensal_laudos, 0) if media_mensal_laudos else 'N/A'}

### Performance vs Meta
- **Taxa de Convers√£o Atual:** {format_number(taxa_conversao, 1) if taxa_conversao else 'N/A'}%
- **Meta de Convers√£o:** 70%
- **Status:** {("üü¢ Acima da Meta" if taxa_conversao and taxa_conversao >= 70 else "üü° Pr√≥ximo √† Meta" if taxa_conversao and taxa_conversao >= 60 else "üî¥ Abaixo da Meta")}

## üìä AN√ÅLISE TEMPORAL

### Tend√™ncia do Per√≠odo
- **Dire√ß√£o:** {monthly_trend}
- **Atendimentos:** {format_number(crescimento_atendimentos, 1) if crescimento_atendimentos else 'N/A'}% de varia√ß√£o
- **Laudos:** {format_number(crescimento_laudos, 1) if crescimento_laudos else 'N/A'}% de varia√ß√£o

## üèÜ TOP PERFORMERS

### Top 10 Unidades Produtivas
{chr(10).join(top_unidades) if top_unidades else "Dados n√£o dispon√≠veis"}

## üìã RECOMENDA√á√ïES

### A√ß√µes para Otimiza√ß√£o
1. **Manter Momentum:** Unidades com alta produtividade devem servir como benchmark
2. **Capacita√ß√£o:** Implementar treinamentos nas unidades com baixa convers√£o
3. **Monitoramento:** Acompanhamento semanal de metas por unidade

---
*Relat√≥rio de Produ√ß√£o - Dashboard PCI/SC v3.0*
"""
    
    def generate_performance_report() -> str:
        """Gera relat√≥rio de performance e efici√™ncia"""
        timestamp = datetime.now().strftime("%d/%m/%Y %H:%M:%S")
        
        # An√°lise de efici√™ncia
        efficiency_insights = []
        
        if taxa_conversao:
            benchmark_diff = taxa_conversao - 70  # Meta de 70%
            if benchmark_diff > 0:
                efficiency_insights.append(f"‚úÖ Taxa de convers√£o {format_number(benchmark_diff,1)}% acima da meta")
            else:
                efficiency_insights.append(f"‚ö†Ô∏è Taxa de convers√£o {format_number(abs(benchmark_diff),1)}% abaixo da meta")
        
        if backlog_meses:
            if backlog_meses <= 3:
                efficiency_insights.append("‚úÖ Backlog dentro do prazo aceit√°vel")
            elif backlog_meses <= 6:
                efficiency_insights.append("‚ö†Ô∏è Backlog requer aten√ß√£o")
            else:
                efficiency_insights.append("üö® Backlog em n√≠vel cr√≠tico")
        
        return f"""# üéØ RELAT√ìRIO DE PERFORMANCE PCI/SC

**Data:** {timestamp}  
**Per√≠odo:** {period_filter}

## üìä INDICADORES DE EFICI√äNCIA

### KPIs Principais
| Indicador | Valor Atual | Meta | Status |
|-----------|-------------|------|--------|
| Taxa de Convers√£o | {format_number(taxa_conversao,1) if taxa_conversao else 'N/A'}% | 70% | {("üü¢" if taxa_conversao and taxa_conversao >= 70 else "üü°" if taxa_conversao and taxa_conversao >= 50 else "üî¥")} |
| Backlog (meses) | {format_number(backlog_meses,1) if backlog_meses else 'N/A'} | < 3 meses | {("üü¢" if backlog_meses and backlog_meses <= 3 else "üü°" if backlog_meses and backlog_meses <= 6 else "üî¥")} |
| Aging M√©dio | {format_number(aging_laudos.get("media_dias") or aging_exames.get("media_dias"), 0) if (aging_laudos or aging_exames) else 'N/A'} dias | < 60 dias | - |

## üîç AN√ÅLISE DE EFICI√äNCIA

### Insights Identificados
{chr(10).join(efficiency_insights) if efficiency_insights else "An√°lise n√£o dispon√≠vel com os dados atuais"}

### Tend√™ncias de Performance
- **Produtividade:** {format_number(media_mensal_laudos,0) if media_mensal_laudos else 'N/A'} laudos/m√™s
- **Capacidade vs Demanda:** {("Equilibrada" if taxa_conversao and 60 <= taxa_conversao <= 80 else "Sobrecarga" if taxa_conversao and taxa_conversao > 80 else "Subutilizada")}

## üéØ METAS E OBJETIVOS

### Metas de Curto Prazo (30 dias)
1. Manter taxa de convers√£o acima de 70%
2. Reduzir aging m√©dio para menos de 60 dias
3. Processar 100% dos casos cr√≠ticos (>90 dias)

### Metas de M√©dio Prazo (90 dias)
1. Atingir backlog inferior a 3 meses
2. Implementar monitoramento em tempo real
3. Estabelecer SLA por tipo de per√≠cia

---
*Relat√≥rio de Performance - Dashboard PCI/SC v3.0*
"""
    
    # Interface de gera√ß√£o de relat√≥rios
    if st.button("üìä Gerar Relat√≥rio", type="primary"):
        with st.spinner("Gerando relat√≥rio..."):
            # Sele√ß√£o do conte√∫do baseado no tipo
            if report_type == "Relat√≥rio Executivo Completo":
                report_content = generate_executive_report()
            elif report_type == "Relat√≥rio de Produ√ß√£o":
                report_content = generate_production_report()
            elif report_type == "Relat√≥rio de Performance":
                report_content = generate_performance_report()
            else:
                report_content = f"# {report_type}\n\n*Relat√≥rio em desenvolvimento*\n\nEste tipo de relat√≥rio ser√° implementado em vers√µes futuras do dashboard."
            
            # Exibi√ß√£o do relat√≥rio
            st.markdown("#### üìÑ Visualiza√ß√£o do Relat√≥rio")
            st.markdown(report_content)
            
            # Prepara√ß√£o para download
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename_base = f"{report_type.lower().replace(' ', '_')}_{timestamp}"
            
            if report_format == "Markdown":
                st.download_button(
                    label="üì• Download Relat√≥rio (Markdown)",
                    data=report_content.encode('utf-8'),
                    file_name=f"{filename_base}.md",
                    mime="text/markdown"
                )
            elif report_format == "HTML":
                # Convers√£o b√°sica para HTML
                html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>{report_type} - PCI/SC</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}
        h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}
        h2 {{ color: #34495e; margin-top: 30px; }}
        h3 {{ color: #7f8c8d; }}
        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        th {{ background-color: #f8f9fa; }}
        .metric {{ background: #e8f5e8; padding: 10px; margin: 5px 0; border-radius: 5px; }}
        .alert {{ background: #fff3cd; padding: 10px; margin: 10px 0; border-left: 4px solid #ffc107; }}
    </style>
</head>
<body>
{report_content.replace(chr(10), '<br>').replace('**', '<strong>').replace('**', '</strong>')}
</body>
</html>
"""
                st.download_button(
                    label="üì• Download Relat√≥rio (HTML)",
                    data=html_content.encode('utf-8'),
                    file_name=f"{filename_base}.html",
                    mime="text/html"
                )
            elif report_format == "JSON":
                # Estrutura√ß√£o em JSON
                json_data = {
                    "relatorio": {
                        "tipo": report_type,
                        "data_geracao": datetime.now().isoformat(),
                        "periodo_analise": period_filter,
                        "kpis": {
                            "total_atendimentos": total_atendimentos,
                            "total_laudos": total_laudos,
                            "taxa_conversao": taxa_conversao,
                            "media_mensal_laudos": media_mensal_laudos,
                            "backlog_meses": backlog_meses,
                            "total_pend_laudos": total_pend_laudos,
                            "total_pend_exames": total_pend_exames
                        },
                        "crescimento": {
                            "atendimentos": crescimento_atendimentos,
                            "laudos": crescimento_laudos
                        },
                        "aging": {
                            "laudos": aging_laudos,
                            "exames": aging_exames
                        },
                        "datasets_utilizados": list(dataframes.keys()),
                        "conteudo_completo": report_content
                    }
                }
                
                import json
                st.download_button(
                    label="üì• Download Relat√≥rio (JSON)",
                    data=json.dumps(json_data, indent=2, ensure_ascii=False).encode('utf-8'),
                    file_name=f"{filename_base}.json",
                    mime="application/json"
                )
    
    # Op√ß√µes avan√ßadas de relat√≥rio
    with st.expander("‚öôÔ∏è Configura√ß√µes Avan√ßadas de Relat√≥rio", expanded=False):
        st.markdown("#### üé® Personaliza√ß√£o")
        
        custom_col1, custom_col2 = st.columns(2)
        
        with custom_col1:
            include_charts = st.checkbox("üìä Incluir gr√°ficos", value=False, help="Adicionar gr√°ficos ao relat√≥rio (formato HTML)")
            include_raw_data = st.checkbox("üìã Incluir dados brutos", value=False, help="Anexar tabelas de dados")
            executive_summary_only = st.checkbox("üìù Apenas resumo executivo", value=False, help="Vers√£o condensada")
        
        with custom_col2:
            custom_period = st.selectbox("üìÖ Per√≠odo customizado", ["Usar filtro atual", "√öltimos 30 dias", "√öltimos 90 dias", "Ano fiscal"])
            language = st.selectbox("üåê Idioma", ["Portugu√™s", "English"])
            classification = st.selectbox("üîí Classifica√ß√£o", ["P√∫blico", "Interno", "Restrito"])
        
        st.markdown("#### üìß Envio Autom√°tico")
        auto_email = st.text_input("‚úâÔ∏è Email para envio:", placeholder="exemplo@pci.sc.gov.br")
        if auto_email:
            st.info("‚öôÔ∏è Funcionalidade de envio autom√°tico ser√° implementada em vers√µes futuras")

# ============ RESUMO NA SIDEBAR ============
with st.sidebar.expander("üìä Resumo da Sess√£o", expanded=False):
    # Datasets carregados
    st.markdown("**üìÅ Datasets Ativos:**")
    for name, df in dataframes.items():
        if df is not None and not df.empty:
            filtered_df = filtered_dataframes.get(name, df)
            icon = "üü¢" if not filtered_df.empty else "üü°"
            st.write(f"{icon} {name.replace('_', ' ').title()}: {len(filtered_df):,}".replace(",", "."))
    
    # Filtros aplicados
    active_filters = sum(1 for filters in dimensional_filters.values() if filters)
    st.markdown(f"**üîç Filtros Ativos:** {active_filters}")
    
    # Per√≠odo de an√°lise
    st.markdown(f"**üìÖ Per√≠odo:** {period_filter}")
    
    # Status geral
    if alerts:
        critical_count = len([a for a in alerts if a["type"] == "danger"])
        warning_count = len([a for a in alerts if a["type"] == "warning"])
        st.markdown(f"**üö® Alertas:** {critical_count} cr√≠ticos, {warning_count} aten√ß√£o")
    else:
        st.markdown("**‚úÖ Status:** Normal")

# ============ RODAP√â ============
st.markdown("---")
st.markdown(f"""
<div style='text-align: center; color: #64748b; padding: 30px; background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%); border-radius: 12px; margin-top: 20px;'>
    <h4 style='color: #1e293b; margin-bottom: 16px;'>üè• Dashboard PCI/SC v3.0</h4>
    <p style='margin: 8px 0; font-size: 16px;'><strong>Sistema Avan√ßado de Monitoramento Executivo</strong></p>
    <p style='margin: 8px 0;'>üìä An√°lise de Produ√ß√£o ‚Ä¢ ‚è∞ Gest√£o de Pend√™ncias ‚Ä¢ üìà Indicadores de Performance ‚Ä¢ üìã Controle Operacional</p>
    <div style='margin: 16px 0; padding: 12px; background: rgba(255,255,255,0.7); border-radius: 8px; display: inline-block;'>
        <p style='margin: 4px 0; font-size: 14px;'><strong>üìß Suporte:</strong> equipe-ti@pci.sc.gov.br</p>
        <p style='margin: 4px 0; font-size: 14px;'><strong>üîß Vers√£o:</strong> 3.0.0 - Melhorias em Performance e UX</p>
        <p style='margin: 4px 0; font-size: 12px; color: #7f8c8d;'><em>√öltima atualiza√ß√£o: {datetime.now().strftime("%d/%m/%Y %H:%M:%S")}</em></p>
    </div>
    <p style='margin-top: 16px; font-size: 12px; color: #9ca3af;'>Desenvolvido para otimiza√ß√£o operacional e tomada de decis√£o baseada em dados</p>
</div>
""", unsafe_allow_html=True)import io
import os
import re
from datetime import datetime, timedelta
from typing import Dict, Optional, Tuple, List, Union

import pandas as pd
import numpy as np
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# ============ CONFIGURA√á√ÉO INICIAL ============
st.set_page_config(
    page_title="PCI/SC ‚Äì Dashboard Executivo",
    layout="wide",
    initial_sidebar_state="expanded",
    page_icon="üè•"
)

# === CONFIGURA√á√ïES DE TEMA E ESTILO ===
px.defaults.template = "plotly_white"
px.defaults.width = None
px.defaults.height = 400

CUSTOM_CSS = """
<style>
/* Cards KPI */
.kpi-card {
  background: linear-gradient(135deg, #ffffff 0%, #f8fafc 100%);
  border: 1px solid #e2e8f0;
  border-radius: 16px;
  padding: 16px 20px;
  height: 100%;
  box-shadow: 0 2px 4px rgba(16,24,40,0.08);
  transition: all 0.2s ease;
}
.kpi-card:hover {
  box-shadow: 0 4px 8px rgba(16,24,40,0.12);
  transform: translateY(-1px);
}
.kpi-title {
  font-size: 13px;
  color: #64748b;
  font-weight: 500;
  margin: 0;
  text-transform: uppercase;
  letter-spacing: 0.5px;
}
.kpi-value {
  font-size: 28px;
  font-weight: 700;
  color: #1e293b;
  margin: 4px 0 2px 0;
  line-height: 1.1;
}
.kpi-delta {
  font-size: 12px;
  color: #475569;
  margin-top: 4px;
  font-weight: 500;
}
.section-title {
  margin: 24px 0 12px 0;
  color: #1e293b;
  font-weight: 600;
}
.alert-success { 
  background: #dcfce7; 
  border-left: 4px solid #22c55e; 
  padding: 12px 16px; 
  border-radius: 6px;
  margin: 8px 0;
}
.alert-warning { 
  background: #fef3c7; 
  border-left: 4px solid #f59e0b; 
  padding: 12px 16px; 
  border-radius: 6px;
  margin: 8px 0;
}
.alert-danger { 
  background: #fee2e2; 
  border-left: 4px solid #ef4444; 
  padding: 12px 16px; 
  border-radius: 6px;
  margin: 8px 0;
}
hr { 
  margin: 12px 0 24px 0; 
  border: none; 
  height: 1px; 
  background: linear-gradient(90deg, transparent, #e2e8f0, transparent);
}
/* Melhoria nos gr√°ficos */
.js-plotly-plot {
  border-radius: 8px;
  box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}
</style>
"""
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)

# === FUN√á√ïES UTILIT√ÅRIAS ===
def segment(label, options, default=None, key=None):
    """Wrapper para segmented_control com fallback para radio"""
    try:
        return st.segmented_control(label, options, default=default, key=key)
    except Exception:
        idx = options.index(default) if (default in options) else 0
        return st.radio(label, options, index=idx, horizontal=True, key=key)

def format_number(value: Union[float, int], decimal_places: int = 0) -> str:
    """Formata n√∫meros com separadores brasileiros"""
    if pd.isna(value) or value is None:
        return "‚Äî"
    try:
        if decimal_places == 0:
            return f"{int(round(value)):,}".replace(",", ".")
        else:
            return f"{value:,.{decimal_places}f}".replace(",", "X").replace(".", ",").replace("X", ".")
    except (ValueError, TypeError):
        return "‚Äî"

def calculate_percentage(numerator: float, denominator: float) -> Optional[float]:
    """Calcula percentual com verifica√ß√£o de divis√£o por zero"""
    if pd.isna(numerator) or pd.isna(denominator) or denominator == 0:
        return None
    return (numerator / denominator) * 100

def kpi_card(title: str, value: str, delta: Optional[str] = None, help_text: Optional[str] = None):
    """Cria card KPI estilizado"""
    delta_html = f'<p class="kpi-delta">{delta}</p>' if delta else ''
    html = f"""
    <div class="kpi-card">
      <p class="kpi-title">{title}</p>
      <p class="kpi-value">{value}</p>
      {delta_html}
    </div>
    """
    st.markdown(html, unsafe_allow_html=True)

# === HEADER ESTILIZADO ===
colh1, colh2 = st.columns([0.7, 0.3])
with colh1:
    st.markdown("<h1 style='margin-bottom:8px'>üè• Dashboard PCI/SC</h1>", unsafe_allow_html=True)
    st.markdown("<p style='color:#64748b; font-size:16px; margin:0;'>Sistema Executivo de Monitoramento ‚Ä¢ Produ√ß√£o ‚Ä¢ Pend√™ncias ‚Ä¢ Performance</p>", unsafe_allow_html=True)
with colh2:
    current_time = datetime.now().strftime("%d/%m/%Y %H:%M")
    st.markdown(f"""
    <div style="display:flex; gap:8px; justify-content:flex-end; align-items:center;">
      <div class="kpi-card" style="padding:8px 12px; text-align:center;">
        <span class="kpi-title">Vers√£o</span>
        <div class="kpi-value" style="font-size:18px;">3.0</div>
      </div>
      <div class="kpi-card" style="padding:8px 12px; text-align:center;">
        <span class="kpi-title">Atualizado</span>
        <div class="kpi-value" style="font-size:14px;">{current_time}</div>
      </div>
    </div>
    """, unsafe_allow_html=True)
st.markdown("<hr/>", unsafe_allow_html=True)

# ============ CACHE E PERFORMANCE ============
@st.cache_data(ttl=3600, show_spinner="Processando dados...")
def read_csv_enhanced(file_content: bytes, filename: str) -> Optional[pd.DataFrame]:
    """L√™ CSV com detec√ß√£o autom√°tica melhorada de separador e encoding"""
    separators = [";", ",", "\t", "|"]
    encodings = ["utf-8", "latin-1", "cp1252", "iso-8859-1"]
    
    for encoding in encodings:
        for sep in separators:
            try:
                bio = io.BytesIO(file_content)
                # Primeira tentativa com configura√ß√µes b√°sicas
                df = pd.read_csv(bio, sep=sep, encoding=encoding, engine="python")
                
                # Verifica√ß√£o de qualidade do parsing
                if df.shape[1] > 1 and len(df) > 0:
                    # Limpeza de aspas e espa√ßos
                    df.columns = [col.strip().strip('"').strip() for col in df.columns]
                    
                    # Limpeza dos dados
                    for col in df.columns:
                        if df[col].dtype == 'object':
                            df[col] = df[col].astype(str).str.strip().str.strip('"')
                            # Convers√£o de valores num√©ricos mascarados como string
                            if col in ['idatendimento', 'iddocumento', 'quantidade']:
                                df[col] = pd.to_numeric(df[col], errors='coerce')
                    
                    return df
            except Exception as e:
                continue
    
    # Fallback para detec√ß√£o autom√°tica mais agressiva
    try:
        bio = io.BytesIO(file_content)
        df = pd.read_csv(bio, sep=None, engine="python", encoding="utf-8", 
                        quotechar='"', skipinitialspace=True)
        if df.shape[1] > 1:
            df.columns = [col.strip().strip('"') for col in df.columns]
            return df
    except Exception:
        pass
    
    st.error(f"‚ùå N√£o foi poss√≠vel processar o arquivo {filename}")
    return None

@st.cache_data(ttl=3600)
def process_datetime_enhanced(series: pd.Series, dayfirst: bool = True) -> Optional[pd.Series]:
    """Processamento aprimorado de datas com m√∫ltiplos formatos"""
    if series is None or len(series) == 0:
        return None
    
    # Lista de formatos comuns para tentar
    date_formats = [
        "%Y-%m-%d",      # ISO format
        "%d/%m/%Y",      # Brazilian format
        "%m/%d/%Y",      # US format
        "%d-%m-%Y",      # European format
        "%Y/%m/%d",      # Alternative ISO
        "%d.%m.%Y",      # German format
    ]
    
    # Primeira tentativa com infer√™ncia autom√°tica
    dt_series = pd.to_datetime(series, errors="coerce", dayfirst=dayfirst, infer_datetime_format=True)
    
    # Se muitos valores falharam, tentar formatos espec√≠ficos
    if dt_series.isna().sum() > len(dt_series) * 0.3:
        for fmt in date_formats:
            try:
                dt_test = pd.to_datetime(series, format=fmt, errors="coerce")
                if dt_test.notna().sum() > dt_series.notna().sum():
                    dt_series = dt_test
                    break
            except Exception:
                continue
    
    return dt_series if dt_series.notna().any() else None

# ============ MAPEAMENTO DE COLUNAS MELHORADO ============
ENHANCED_COLUMN_MAPPINGS = {
    "detalhes_laudospendentes": {
        "date_columns": ["data_solicitacao"],
        "id_column": "caso_sirsaelp",
        "dimensions": {
            "unidade": "unidade",
            "superintendencia": "superintendencia", 
            "diretoria": "diretoria",
            "tipo": "tipopericia",
            "perito": "perito",
            "competencia": "competencia"
        }
    },
    "detalhes_examespendentes": {
        "date_columns": ["data_solicitacao"],
        "id_column": "caso_sirsaelp",
        "dimensions": {
            "unidade": "unidade",
            "superintendencia": "superintendencia",
            "diretoria": "diretoria", 
            "tipo": "tipopericia",
            "competencia": "competencia"
        }
    },
    "Atendimentos_todos_Mensal": {
        "date_columns": ["data_interesse"],
        "id_column": "idatendimento",
        "quantity_column": "idatendimento",
        "aggregation_level": "monthly"
    },
    "Laudos_todos_Mensal": {
        "date_columns": ["data_interesse"],
        "id_column": "iddocumento", 
        "quantity_column": "iddocumento",
        "aggregation_level": "monthly"
    },
    "Atendimentos_especifico_Mensal": {
        "date_columns": ["data_interesse"],
        "id_column": "idatendimento",
        "quantity_column": "idatendimento",
        "dimensions": {"tipo": "txcompetencia"},
        "aggregation_level": "monthly"
    },
    "Laudos_especifico_Mensal": {
        "date_columns": ["data_interesse"],
        "id_column": "iddocumento",
        "quantity_column": "iddocumento", 
        "dimensions": {"tipo": "txcompetencia"},
        "aggregation_level": "monthly"
    },
    "Atendimentos_diario": {
        "date_columns": ["data_interesse"],
        "id_column": "idatendimento",
        "quantity_column": "idatendimento",
        "aggregation_level": "daily"
    },
    "Laudos_diario": {
        "date_columns": ["data_interesse"],
        "id_column": "iddocumento",
        "quantity_column": "iddocumento", 
        "aggregation_level": "daily"
    }
}

# ============ PADRONIZA√á√ÉO MELHORADA ============
@st.cache_data(ttl=3600)
def standardize_dataframe_enhanced(name: str, df: pd.DataFrame) -> pd.DataFrame:
    """Padroniza√ß√£o aprimorada com mapeamento flex√≠vel"""
    if df is None or df.empty:
        return pd.DataFrame()
    
    result = df.copy()
    mapping = ENHANCED_COLUMN_MAPPINGS.get(name, {})
    
    # Normaliza√ß√£o de nomes de colunas
    result.columns = [col.lower().strip().replace(' ', '_') for col in result.columns]
    
    # Processamento de quantidade
    quantity_col = mapping.get("quantity_column")
    if quantity_col and quantity_col in result.columns:
        result["quantidade"] = pd.to_numeric(result[quantity_col], errors="coerce").fillna(1)
    else:
        result["quantidade"] = 1
    
    # Processamento de dimens√µes
    dimensions = mapping.get("dimensions", {})
    for target_col, source_col in dimensions.items():
        if source_col in result.columns:
            result[target_col] = result[source_col].astype(str).str.strip().str.title()
    
    # Processamento de datas
    date_columns = mapping.get("date_columns", [])
    for date_col in date_columns:
        if date_col in result.columns:
            processed_date = process_datetime_enhanced(result[date_col])
            if processed_date is not None:
                result["data_base"] = processed_date
                
                # Criar campos derivados de data
                result["anom√™s_dt"] = processed_date.dt.to_period("M").dt.to_timestamp()
                result["anom√™s"] = result["anom√™s_dt"].dt.strftime("%Y-%m")
                result["ano"] = result["anom√™s_dt"].dt.year
                result["mes"] = result["anom√™s_dt"].dt.month
                result["dia"] = processed_date.dt.normalize()
                break
    
    # Processamento de ID √∫nico
    id_col = mapping.get("id_column")
    if id_col and id_col in result.columns:
        result["id"] = result[id_col].astype(str)
    
    # Limpeza final de dados categ√≥ricos
    categorical_cols = ["diretoria", "superintendencia", "unidade", "tipo", "perito", "competencia"]
    for col in categorical_cols:
        if col in result.columns:
            result[col] = (result[col].astype(str)
                          .str.strip()
                          .str.title()
                          .replace({"Nan": None, "": None, "None": None}))
    
    return result

# ============ DETEC√á√ÉO E CARREGAMENTO DE ARQUIVOS ============
def detect_data_sources() -> bool:
    """Detecta se existem arquivos na pasta data/"""
    return os.path.exists("data") and any(f.endswith(".csv") for f in os.listdir("data"))

def get_file_configs() -> Dict[str, Dict]:
    """Configura√ß√µes dos arquivos esperados"""
    return {
        "Atendimentos_todos_Mensal": {
            "label": "Atendimentos Todos (Mensal)",
            "description": "Dados agregados de atendimentos por m√™s",
            "pattern": ["atendimentos_todos", "atendimentos todos"]
        },
        "Laudos_todos_Mensal": {
            "label": "Laudos Todos (Mensal)", 
            "description": "Dados agregados de laudos por m√™s",
            "pattern": ["laudos_todos", "laudos todos"]
        },
        "Atendimentos_especifico_Mensal": {
            "label": "Atendimentos Espec√≠ficos (Mensal)",
            "description": "Atendimentos detalhados por tipo e compet√™ncia",
            "pattern": ["atendimentos_especifico", "atendimentos especifico"]
        },
        "Laudos_especifico_Mensal": {
            "label": "Laudos Espec√≠ficos (Mensal)",
            "description": "Laudos detalhados por tipo e compet√™ncia", 
            "pattern": ["laudos_especifico", "laudos especifico"]
        },
        "Atendimentos_diario": {
            "label": "Atendimentos (Di√°rio)",
            "description": "Registros di√°rios de atendimentos",
            "pattern": ["atendimentos_diario", "atendimentos_di√°rio", "atendimentos di√°rio"]
        },
        "Laudos_diario": {
            "label": "Laudos (Di√°rio)",
            "description": "Registros di√°rios de laudos", 
            "pattern": ["laudos_diario", "laudos_di√°rio", "laudos di√°rio"]
        },
        "detalhes_laudospendentes": {
            "label": "Laudos Pendentes",
            "description": "Detalhes de laudos aguardando conclus√£o",
            "pattern": ["laudospendentes", "laudos_pendentes", "detalhes_laudospendentes"]
        },
        "detalhes_examespendentes": {
            "label": "Exames Pendentes", 
            "description": "Detalhes de exames aguardando realiza√ß√£o",
            "pattern": ["examespendentes", "exames_pendentes", "detalhes_examespendentes"]
        }
    }

def resolve_file_path(name: str, file_configs: Dict) -> Optional[str]:
    """Resolve caminho do arquivo com toler√¢ncia a varia√ß√µes de nome"""
    if not os.path.exists("data"):
        return None
    
    config = file_configs.get(name, {})
    patterns = config.get("pattern", [name.lower().replace(" ", "_")])
    patterns.append(name.lower().replace(" ", "_"))
    
    for filename in os.listdir("data"):
        if not filename.lower().endswith(".csv"):
            continue
            
        base_name = os.path.splitext(filename)[0].lower()
        normalized_name = re.sub(r"[^\w]", "_", base_name)
        
        for pattern in patterns:
            pattern_normalized = re.sub(r"[^\w]", "_", pattern)
            if pattern_normalized in normalized_name or normalized_name.startswith(pattern_normalized):
                return os.path.join("data", filename)
    
    return None

# ============ INTERFACE DE UPLOAD E CARREGAMENTO ============
st.sidebar.header("üìÅ Dados do Sistema")
has_data_dir = detect_data_sources()
file_configs = get_file_configs()

if not has_data_dir:
    st.sidebar.info("üí° Upload dos arquivos CSV para an√°lise")

# Upload de arquivos
uploads = {}
for key, config in file_configs.items():
    if not has_data_dir:
        uploads[key] = st.sidebar.file_uploader(
            f"{config['label']} (.csv)",
            help=config['description'],
            key=f"upload_{key}",
            type=['csv']
        )
    else:
        uploads[key] = None

@st.cache_data(ttl=3600, show_spinner="Carregando dados...")
def load_all_data_enhanced(file_sources: Dict) -> Dict[str, pd.DataFrame]:
    """Carregamento otimizado de todos os dados"""
    loaded_data = {}
    loading_stats = []
    
    for name, upload_file in file_sources.items():
        df = None
        
        # Carregamento de pasta local ou upload
        if has_data_dir:
            file_path = resolve_file_path(name, file_configs)
            if file_path and os.path.exists(file_path):
                try:
                    with open(file_path, 'rb') as f:
                        content = f.read()
                    df = read_csv_enhanced(content, name)
                    
                    if df is not None:
                        loading_stats.append(f"‚úÖ {name}: {len(df):,} registros".replace(",", "."))
                except Exception as e:
                    loading_stats.append(f"‚ùå {name}: Erro - {str(e)}")
        else:
            if upload_file is not None:
                try:
                    content = upload_file.read()
                    df = read_csv_enhanced(content, name)
                    
                    if df is not None:
                        loading_stats.append(f"‚úÖ {name}: {len(df):,} registros".replace(",", "."))
                except Exception as e:
                    loading_stats.append(f"‚ùå {name}: Erro - {str(e)}")
        
        # Padroniza√ß√£o dos dados carregados
        if df is not None:
            standardized_df = standardize_dataframe_enhanced(name, df)
            if not standardized_df.empty:
                loaded_data[name] = standardized_df
    
    # Exibir estat√≠sticas de carregamento
    for stat in loading_stats:
        if "‚úÖ" in stat:
            st.sidebar.success(stat)
        else:
            st.sidebar.error(stat)
    
    return loaded_data

# Carregamento dos dados
with st.spinner("Processando e padronizando dados..."):
    dataframes = load_all_data_enhanced(uploads)

if not dataframes:
    st.warning("‚ö†Ô∏è Nenhum arquivo foi carregado com sucesso.")
    st.info("üìù **Arquivos esperados:** " + ", ".join(file_configs.keys()))
    st.info("üîß **Formatos suportados:** CSV com separadores `;`, `,`, `|` ou tab")
    st.stop()

# ============ FILTROS APRIMORADOS ============
def extract_filter_values_enhanced(column: str) -> List[str]:
    """Extra√ß√£o aprimorada de valores √∫nicos para filtros"""
    values = set()
    for df in dataframes.values():
        if column in df.columns:
            unique_vals = df[column].dropna().astype(str).unique()
            values.update(v for v in unique_vals if v and v.lower() not in ["nan", "none", ""])
    return sorted(list(values))

def apply_filters_enhanced(df: pd.DataFrame, filters: Dict) -> pd.DataFrame:
    """Aplica√ß√£o otimizada de filtros"""
    if df is None or df.empty:
        return pd.DataFrame()
    
    filtered = df.copy()
    
    # Filtros dimensionais
    for column, filter_values in filters.get("dimensions", {}).items():
        if column in filtered.columns and filter_values:
            filtered = filtered[filtered[column].astype(str).isin(filter_values)]
    
    # Filtro temporal
    period_filter = filters.get("period")
    if "anom√™s_dt" in filtered.columns and period_filter != "Todo o per√≠odo":
        max_date = filtered["anom√™s_dt"].max()
        if pd.notna(max_date):
            cutoff_map = {
                "√öltimos 3 meses": pd.DateOffset(months=3),
                "√öltimos 6 meses": pd.DateOffset(months=6), 
                "√öltimo ano": pd.DateOffset(years=1),
                "Ano atual": None
            }
            
            if period_filter == "Ano atual":
                cutoff_date = pd.Timestamp(max_date.year, 1, 1)
            else:
                offset = cutoff_map.get(period_filter)
                cutoff_date = max_date - offset if offset else None
            
            if cutoff_date is not None:
                filtered = filtered[filtered["anom√™s_dt"] >= cutoff_date]
    
    return filtered

# Configura√ß√£o de filtros na sidebar
st.sidebar.subheader("üîç Filtros de An√°lise")

# Filtros dimensionais
dimensional_filters = {}
dimensional_filters["diretoria"] = st.sidebar.multiselect(
    "üè¢ Diretoria", 
    extract_filter_values_enhanced("diretoria"),
    help="Filtrar por diretoria espec√≠fica"
)
dimensional_filters["superintendencia"] = st.sidebar.multiselect(
    "üèõÔ∏è Superintend√™ncia", 
    extract_filter_values_enhanced("superintendencia"),
    help="Filtrar por superintend√™ncia"
)
dimensional_filters["unidade"] = st.sidebar.multiselect(
    "üè™ Unidade", 
    extract_filter_values_enhanced("unidade"),
    help="Filtrar por unidade operacional"
)
dimensional_filters["tipo"] = st.sidebar.multiselect(
    "üî¨ Tipo de Per√≠cia", 
    extract_filter_values_enhanced("tipo"),
    help="Filtrar por tipo de per√≠cia"
)

# Filtro temporal
period_options = ["Todo o per√≠odo", "Ano atual", "√öltimos 6 meses", "√öltimos 3 meses"]
period_filter = st.sidebar.selectbox(
    "üìÖ Per√≠odo de An√°lise", 
    period_options,
    help="Selecionar per√≠odo temporal para an√°lise"
)

# Configura√ß√µes de visualiza√ß√£o
st.sidebar.subheader("‚öôÔ∏è Configura√ß√µes")
show_benchmarks = st.sidebar.toggle(
    "üìä Exibir Metas", 
    value=True,
    help="Mostrar linhas de refer√™ncia e metas nos gr√°ficos"
)
chart_height = st.sidebar.slider(
    "üìè Altura dos Gr√°ficos", 
    min_value=300, 
    max_value=600, 
    value=400,
    help="Ajustar altura padr√£o dos gr√°ficos"
)

# Consolida√ß√£o dos filtros
all_filters = {
    "dimensions": dimensional_filters,
    "period": period_filter
}

# Aplica√ß√£o dos filtros
filtered_dataframes = {
    name: apply_filters_enhanced(df, all_filters) 
    for name, df in dataframes.items()
}

# Atalhos para datasets principais
df_atend_todos = filtered_dataframes.get("Atendimentos_todos_Mensal")
df_laudos_todos = filtered_dataframes.get("Laudos_todos_Mensal")
df_atend_esp = filtered_dataframes.get("Atendimentos_especifico_Mensal")
df_laudos_esp = filtered_dataframes.get("Laudos_especifico_Mensal")
df_atend_diario = filtered_dataframes.get("Atendimentos_diario")
df_laudos_diario = filtered_dataframes.get("Laudos_diario")
df_pend_laudos = filtered_dataframes.get("detalhes_laudospendentes")
df_pend_exames = filtered_dataframes.get("detalhes_examespendentes")

# ============ C√ÅLCULOS DE KPIS APRIMORADOS ============
class KPICalculator:
    """Classe para c√°lculos padronizados de KPIs"""
    
    @staticmethod
    def calculate_total(df: pd.DataFrame) -> int:
        """Calcula total de registros/quantidade"""
        if df is None or df.empty or "quantidade" not in df.columns:
            return 0
        return int(df["quantidade"].sum())
    
    @staticmethod
    def calculate_monthly_average(df: pd.DataFrame) -> Optional[float]:
        """Calcula m√©dia mensal"""
        if df is None or df.empty or "anom√™s_dt" not in df.columns:
            return None
        monthly_totals = df.groupby("anom√™s_dt")["quantidade"].sum()
        return monthly_totals.mean() if len(monthly_totals) > 0 else None
    
    @staticmethod
    def calculate_growth_rate(df: pd.DataFrame, periods: int = 3) -> Optional[float]:
        """Calcula taxa de crescimento entre per√≠odos"""
        if df is None or df.empty or "anom√™s_dt" not in df.columns:
            return None
        
        monthly_data = df.groupby("anom√™s_dt")["quantidade"].sum().sort_index()
        if len(monthly_data) < periods * 2:
            return None
        
        recent_data = monthly_data.tail(periods * 2)
        mid_point = len(recent_data) // 2
        first_half = recent_data.iloc[:mid_point].mean()
        second_half = recent_data.iloc[mid_point:].mean()
        
        if first_half > 0:
            return ((second_half - first_half) / first_half) * 100
        return None
    
    @staticmethod
    def calculate_conversion_rate(df_input: pd.DataFrame, df_output: pd.DataFrame) -> Optional[float]:
        """Calcula taxa de convers√£o entre dois datasets"""
        if df_input is None or df_output is None:
            return None
        
        total_input = KPICalculator.calculate_total(df_input)
        total_output = KPICalculator.calculate_total(df_output)
        
        return calculate_percentage(total_output, total_input)
    
    @staticmethod
    def calculate_aging_stats(df: pd.DataFrame, date_column: str = "data_base") -> Dict:
        """Calcula estat√≠sticas de aging para pend√™ncias"""
        if df is None or df.empty or date_column not in df.columns:
            return {}
        
        dates = pd.to_datetime(df[date_column], errors="coerce")
        if dates.isna().all():
            return {}
        
        hoje = pd.Timestamp.now().normalize()
        aging_days = (hoje - dates).dt.days
        
        return {
            "total": len(df),
            "media_dias": float(aging_days.mean()),
            "mediana_dias": float(aging_days.median()),
            "max_dias": int(aging_days.max()),
            "p75_dias": float(aging_days.quantile(0.75)),
            "p90_dias": float(aging_days.quantile(0.90))
        }

# C√°lculo dos KPIs principais
calc = KPICalculator()

# KPIs de Produ√ß√£o
total_atendimentos = calc.calculate_total(df_atend_todos)
total_laudos = calc.calculate_total(df_laudos_todos)
media_mensal_atendimentos = calc.calculate_monthly_average(df_atend_todos)
media_mensal_laudos = calc.calculate_monthly_average(df_laudos_todos)
taxa_conversao = calc.calculate_conversion_rate(df_atend_todos, df_laudos_todos)
crescimento_atendimentos = calc.calculate_growth_rate(df_atend_todos)
crescimento_laudos = calc.calculate_growth_rate(df_laudos_todos)

# KPIs de Pend√™ncias
total_pend_laudos = len(df_pend_laudos) if df_pend_laudos is not None else 0
total_pend_exames = len(df_pend_exames) if df_pend_exames is not None else 0
aging_laudos = calc.calculate_aging_stats(df_pend_laudos)
aging_exames = calc.calculate_aging_stats(df_pend_exames)

# Estimativa de backlog
backlog_meses = None
if media_mensal_laudos and media_mensal_laudos > 0:
    backlog_meses = total_pend_laudos / media_mensal_laudos

# ============ FILTROS R√ÅPIDOS ============
st.markdown("<h4 class='section-title'>üéõÔ∏è Filtros R√°pidos</h4>", unsafe_allow_html=True)
fc1, fc2, fc3, fc4 = st.columns([0.3, 0.25, 0.25, 0.2])

with fc1:
    quick_period = segment(
        "üìÖ Per√≠odo", 
        ["Todo o per√≠odo", "Ano atual", "√öltimos 6 meses", "√öltimos 3 meses"],
        default=period_filter, 
        key="quick_period"
    )

with fc2:
    view_mode = segment(
        "üëÅÔ∏è Visualiza√ß√£o", 
        ["Resumo", "Detalhado", "Comparativo"],
        default="Resumo", 
        key="view_mode"
    )

with fc3:
    analysis_focus = segment(
        "üéØ Foco", 
        ["Produ√ß√£o", "Pend√™ncias", "Performance"],
        default="Produ√ß√£o", 
        key="analysis_focus"
    )

with fc4:
    export_format = st.selectbox(
        "üì• Exportar",
        ["Nenhum", "PDF", "Excel", "CSV"],
        key="export_format"
    )

# ============ DASHBOARD PRINCIPAL - KPIS ============
st.markdown("<h4 class='section-title'>üìà Indicadores Principais</h4>", unsafe_allow_html=True)

# Linha 1 - Produ√ß√£o
c1, c2, c3, c4 = st.columns(4)
with c1:
    delta_atend = f"‚ÜóÔ∏è {format_number(crescimento_atendimentos,1)}%" if crescimento_atendimentos and crescimento_atendimentos > 0 else f"‚ÜòÔ∏è {format_number(abs(crescimento_atendimentos or 0),1)}%" if crescimento_atendimentos else None
    kpi_card("Atendimentos Totais", format_number(total_atendimentos), delta_atend)

with c2:
    delta_laudos = f"‚ÜóÔ∏è {format_number(crescimento_laudos,1)}%" if crescimento_laudos and crescimento_laudos > 0 else f"‚ÜòÔ∏è {format_number(abs(crescimento_laudos or 0),1)}%" if crescimento_laudos else None
    kpi_card("Laudos Emitidos", format_number(total_laudos), delta_laudos)

with c3:
    taxa_str = f"{format_number(taxa_conversao,1)}%" if taxa_conversao else "‚Äî"
    taxa_color = "üü¢" if taxa_conversao and taxa_conversao >= 70 else "üü°" if taxa_conversao and taxa_conversao >= 50 else "üî¥"
    kpi_card("Taxa de Convers√£o", f"{taxa_color} {taxa_str}")

with c4:
    prod_str = f"{format_number(media_mensal_laudos,0)}" if media_mensal_laudos else "‚Äî"
    kpi_card("Produtividade Mensal", f"{prod_str} laudos/m√™s")

# Linha 2 - Pend√™ncias e Performance
st.markdown("<h4 class='section-title'>‚è∞ Gest√£o de Pend√™ncias</h4>", unsafe_allow_html=True)
c5, c6, c7, c8 = st.columns(4)

with c5:
    pend_color = "üî¥" if total_pend_laudos > 1000 else "üü°" if total_pend_laudos > 500 else "üü¢"
    kpi_card("Laudos Pendentes", f"{pend_color} {format_number(total_pend_laudos)}")

with c6:
    exam_color = "üî¥" if total_pend_exames > 2000 else "üü°" if total_pend_exames > 1000 else "üü¢"
    kpi_card("Exames Pendentes", f"{exam_color} {format_number(total_pend_exames)}")

with c7:
    backlog_str = f"{format_number(backlog_meses,1)} meses" if backlog_meses else "‚Äî"
    backlog_color = "üî¥" if backlog_meses and backlog_meses > 6 else "üü°" if backlog_meses and backlog_meses > 3 else "üü¢"
    kpi_card("Backlog Estimado", f"{backlog_color} {backlog_str}")

with c8:
    aging_medio = aging_laudos.get("media_dias") or aging_exames.get("media_dias")
    aging_str = f"{format_number(aging_medio,0)} dias" if aging_medio else "‚Äî"
    aging_color = "üî¥" if aging_medio and aging_medio > 90 else "üü°" if aging_medio and aging_medio > 60 else "üü¢"
    kpi_card("Aging M√©dio", f"{aging_color} {aging_str}")

# ============ ALERTAS INTELIGENTES ============
st.markdown("<h4 class='section-title'>üö® Alertas e Insights</h4>", unsafe_allow_html=True)

def generate_smart_alerts() -> List[Dict]:
    """Gera alertas inteligentes baseados nos KPIs"""
    alerts = []
    
    # Alertas cr√≠ticos
    if backlog_meses and backlog_meses > 6:
        alerts.append({
            "type": "danger",
            "icon": "üî¥",
            "title": "BACKLOG CR√çTICO",
            "message": f"Backlog de {format_number(backlog_meses,1)} meses excede limite seguro (6 meses)"
        })
    
    if taxa_conversao and taxa_conversao < 50:
        alerts.append({
            "type": "danger", 
            "icon": "üî¥",
            "title": "EFICI√äNCIA BAIXA",
            "message": f"Taxa de convers√£o de {format_number(taxa_conversao,1)}% abaixo do m√≠nimo (50%)"
        })
    
    # Alertas de aten√ß√£o
    if crescimento_laudos and crescimento_laudos < -10:
        alerts.append({
            "type": "warning",
            "icon": "üü°", 
            "title": "QUEDA NA PRODU√á√ÉO",
            "message": f"Redu√ß√£o de {format_number(abs(crescimento_laudos),1)}% na emiss√£o de laudos"
        })
    
    if backlog_meses and 3 < backlog_meses <= 6:
        alerts.append({
            "type": "warning",
            "icon": "üü°",
            "title": "BACKLOG ELEVADO", 
            "message": f"Backlog de {format_number(backlog_meses,1)} meses requer aten√ß√£o"
        })
    
    # Alertas positivos
    if crescimento_laudos and crescimento_laudos > 10:
        alerts.append({
            "type": "success",
            "icon": "üü¢",
            "title": "CRESCIMENTO POSITIVO",
            "message": f"Aumento de {format_number(crescimento_laudos,1)}% na produ√ß√£o de laudos"
        })
    
    if taxa_conversao and taxa_conversao >= 80:
        alerts.append({
            "type": "success",
            "icon": "üü¢", 
            "title": "ALTA EFICI√äNCIA",
            "message": f"Taxa de convers√£o de {format_number(taxa_conversao,1)}% acima da meta"
        })
    
    return alerts

alerts = generate_smart_alerts()

if alerts:
    # Organizar alertas por tipo
    critical_alerts = [a for a in alerts if a["type"] == "danger"]
    warning_alerts = [a for a in alerts if a["type"] == "warning"] 
    success_alerts = [a for a in alerts if a["type"] == "success"]
    
    alert_cols = st.columns(len(alerts))
    for i, alert in enumerate(alerts):
        with alert_cols[i]:
            css_class = f"alert-{alert['type']}"
            st.markdown(f"""
            <div class="{css_class}">
                <strong>{alert['icon']} {alert['title']}</strong><br>
                {alert['message']}
            </div>
            """, unsafe_allow_html=True)
else:
    st.markdown("""
    <div class="alert-success">
        <strong>‚úÖ SITUA√á√ÉO NORMAL</strong><br>
        Todos os indicadores est√£o dentro dos par√¢metros esperados
    </div>
    """, unsafe_allow_html=True)

st.markdown("<hr/>", unsafe_allow_html=True)

# ============ NAVEGA√á√ÉO POR ABAS ============
tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
    "üìä Vis√£o Geral",
    "üìà Tend√™ncias",
    "üèÜ Rankings", 
    "‚è∞ Pend√™ncias",
    "üìÖ An√°lise Di√°ria",
    "üìã Dados Brutos",
    "üìë Relat√≥rios"
])

# ============ ABA 1: VIS√ÉO GERAL ============
with tab1:
    st.subheader("üìä Panorama Executivo")
    
    # Gr√°fico principal - Evolu√ß√£o temporal
    if df_atend_todos is not None and df_laudos_todos is not None:
        col_chart1, col_chart2 = st.columns([0.7, 0.3])
        
        with col_chart1:
            st.markdown("#### üìà Evolu√ß√£o Mensal: Atendimentos vs Laudos")
            
            # Prepara√ß√£o dos dados
            atend_monthly = df_atend_todos.groupby("anom√™s_dt")["quantidade"].sum().reset_index()
            laudos_monthly = df_laudos_todos.groupby("anom√™s_dt")["quantidade"].sum().reset_index()
            
            # Gr√°fico combinado
            fig_evolution = go.Figure()
            
            fig_evolution.add_trace(go.Scatter(
                x=atend_monthly["anom√™s_dt"],
                y=atend_monthly["quantidade"], 
                mode='lines+markers',
                name='Atendimentos',
                line=dict(color='#3b82f6', width=3),
                marker=dict(size=6)
            ))
            
            fig_evolution.add_trace(go.Scatter(
                x=laudos_monthly["anom√™s_dt"],
                y=laudos_monthly["quantidade"],
                mode='lines+markers', 
                name='Laudos',
                line=dict(color='#10b981', width=3),
                marker=dict(size=6)
            ))
            
            # Linhas de tend√™ncia se solicitado
            if show_benchmarks and len(atend_monthly) > 3:
                # M√©dia m√≥vel simples
                atend_monthly['ma3'] = atend_monthly['quantidade'].rolling(3).mean()
                laudos_monthly['ma3'] = laudos_monthly['quantidade'].rolling(3).mean()
                
                fig_evolution.add_trace(go.Scatter(
                    x=atend_monthly["anom√™s_dt"],
                    y=atend_monthly["ma3"],
                    mode='lines',
                    name='Tend. Atendimentos',
                    line=dict(color='#3b82f6', width=2, dash='dash'),
                    showlegend=False
                ))
                
                fig_evolution.add_trace(go.Scatter(
                    x=laudos_monthly["anom√™s_dt"], 
                    y=laudos_monthly["ma3"],
                    mode='lines',
                    name='Tend. Laudos',
                    line=dict(color='#10b981', width=2, dash='dash'),
                    showlegend=False
                ))
            
            fig_evolution.update_layout(
                height=chart_height,
                hovermode='x unified',
                xaxis_title="Per√≠odo",
                yaxis_title="Quantidade",
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
            )
            
            st.plotly_chart(fig_evolution, use_container_width=True)
        
        with col_chart2:
            st.markdown("#### üéØ Taxa de Convers√£o")
            
            # C√°lculo da taxa de convers√£o mensal
            merged_monthly = pd.merge(
                atend_monthly.rename(columns={"quantidade": "Atendimentos"}),
                laudos_monthly.rename(columns={"quantidade": "Laudos"}),
                on="anom√™s_dt",
                how="inner"
            )
            
            if not merged_monthly.empty:
                merged_monthly["Taxa_Conversao"] = (merged_monthly["Laudos"] / merged_monthly["Atendimentos"]) * 100
                
                fig_conversion = go.Figure()
                fig_conversion.add_trace(go.Scatter(
                    x=merged_monthly["anom√™s_dt"],
                    y=merged_monthly["Taxa_Conversao"],
                    mode='lines+markers',
                    line=dict(color='#f59e0b', width=3),
                    marker=dict(size=8),
                    name='Taxa de Convers√£o'
                ))
                
                if show_benchmarks:
                    fig_conversion.add_hline(
                        y=70, 
                        line_dash="dash", 
                        line_color="red",
                        annotation_text="Meta: 70%"
                    )
                
                fig_conversion.update_layout(
                    height=chart_height,
                    xaxis_title="Per√≠odo",
                    yaxis_title="Taxa (%)",
                    yaxis=dict(range=[0, 100]),
                    showlegend=False
                )
                
                st.plotly_chart(fig_conversion, use_container_width=True)
    
    # An√°lises por dimens√£o
    col_dim1, col_dim2 = st.columns(2)
    
    with col_dim1:
        st.markdown("#### üè¢ Performance por Unidade")
        if df_laudos_todos is not None and "unidade" in df_laudos_todos.columns:
            unidade_summary = (
                df_laudos_todos.groupby("unidade")["quantidade"]
                .sum()
                .sort_values(ascending=False)
                .head(15)
                .reset_index()
            )
            
            fig_unidades = px.bar(
                unidade_summary,
                x="quantidade", 
                y="unidade",
                orientation="h",
                title="Top 15 Unidades - Laudos Emitidos",
                color="quantidade",
                color_continuous_scale="Blues"
            )
            
            fig_unidades.update_layout(
                height=500,
                showlegend=False,
                yaxis={'categoryorder': 'total ascending'}
            )
            
            st.plotly_chart(fig_unidades, use_container_width=True)
    
    with col_dim2:
        st.markdown("#### üîç An√°lise Pareto - Tipos de Per√≠cia")
        if df_laudos_esp is not None and "tipo" in df_laudos_esp.columns:
            tipo_summary = (
                df_laudos_esp.groupby("tipo")["quantidade"]
                .sum()
                .sort_values(ascending=False)
                .reset_index()
            )
            
            tipo_summary["pct"] = 100 * tipo_summary["quantidade"] / tipo_summary["quantidade"].sum()
            tipo_summary["pct_acum"] = tipo_summary["pct"].cumsum()
            
            fig_pareto = make_subplots(specs=[[{"secondary_y": True}]])
            
            fig_pareto.add_trace(
                go.Bar(
                    x=tipo_summary["tipo"].head(10),
                    y=tipo_summary["quantidade"].head(10),
                    name="Quantidade",
                    marker_color='lightblue'
                )
            )
            
            fig_pareto.add_trace(
                go.Scatter(
                    x=tipo_summary["tipo"].head(10),
                    y=tipo_summary["pct_acum"].head(10),
                    mode="lines+markers",
                    name="% Acumulado",
                    line=dict(color='red', width=3),
                    marker=dict(size=6)
                ),
                secondary_y=True,
            )
            
            if show_benchmarks:
                fig_pareto.add_hline(
                    y=80, 
                    line_dash="dash", 
                    line_color="red",
                    secondary_y=True,
                    annotation_text="80%"
                )
            
            fig_pareto.update_layout(
                title="Top 10 Tipos de Per√≠cia",
                height=500,
                hovermode="x unified"
            )
            fig_pareto.update_yaxes(title_text="Quantidade", secondary_y=False)
            fig_pareto.update_yaxes(title_text="% Acumulado", range=[0, 100], secondary_y=True)
            
            st.plotly_chart(fig_pareto, use_container_width=True)

# ============ ABA 2: TEND√äNCIAS ============
with tab2:
    st.subheader("üìà An√°lise de Tend√™ncias Avan√ßada")
    
    def create_advanced_time_series(df: pd.DataFrame, title: str, color: str = "blue"):
        """Cria s√©rie temporal avan√ßada com decomposi√ß√£o"""
        if df is None or df.empty or "anom√™s_dt" not in df.columns:
            st.info(f"Dados insuficientes para {title}")
            return
        
        monthly_data = df.groupby("anom√™s_dt")["quantidade"].sum().sort_index()
        if len(monthly_data) < 3:
            st.info(f"Per√≠odo insuficiente para an√°lise de tend√™ncia: {title}")
            return
        
        # Prepara√ß√£o dos dados
        dates = monthly_data.index
        values = monthly_data.values
        
        # C√°lculos estat√≠sticos
        ma3 = monthly_data.rolling(window=3, center=True).mean()
        pct_change = monthly_data.pct_change() * 100
        
        # Detec√ß√£o de tend√™ncia (regress√£o linear simples)
        from scipy import stats
        x_numeric = np.arange(len(monthly_data))
        slope, intercept, r_value, p_value, std_err = stats.linregress(x_numeric, values)
        trend_line = slope * x_numeric + intercept
        
        # Cria√ß√£o do gr√°fico
        fig = make_subplots(
            rows=3, cols=1,
            subplot_titles=(
                f"{title} - S√©rie Temporal",
                "Varia√ß√£o Percentual Mensal", 
                "Tend√™ncia e Sazonalidade"
            ),
            vertical_spacing=0.08,
            row_heights=[0.5, 0.25, 0.25]
        )
        
        # S√©rie principal
        fig.add_trace(
            go.Scatter(
                x=dates, y=values,
                mode="lines+markers",
                name="Valores Observados",
                line=dict(color=color, width=2),
                marker=dict(size=6)
            ),
            row=1, col=1
        )
        
        # M√©dia m√≥vel
        fig.add_trace(
            go.Scatter(
                x=dates, y=ma3,
                mode="lines",
                name="M√©dia M√≥vel (3m)",
                line=dict(color="red", width=2, dash="dash")
            ),
            row=1, col=1
        )
        
        # Linha de tend√™ncia
        fig.add_trace(
            go.Scatter(
                x=dates, y=trend_line,
                mode="lines",
                name=f"Tend√™ncia (R¬≤={r_value**2:.3f})",
                line=dict(color="orange", width=2, dash="dot")
            ),
            row=1, col=1
        )
        
        # Varia√ß√£o percentual
        colors = ['red' if x < 0 else 'green' for x in pct_change.fillna(0)]
        fig.add_trace(
            go.Bar(
                x=dates, y=pct_change,
                name="Varia√ß√£o %",
                marker_color=colors,
                showlegend=False
            ),
            row=2, col=1
        )
        
        # An√°lise sazonal (se houver dados suficientes)
        if len(monthly_data) >= 12:
            seasonal_pattern = monthly_data.groupby(monthly_data.index.month).mean()
            months = ['Jan', 'Fev', 'Mar', 'Abr', 'Mai', 'Jun',
                     'Jul', 'Ago', 'Set', 'Out', 'Nov', 'Dez']
            
            fig.add_trace(
                go.Bar(
                    x=months[:len(seasonal_pattern)], 
                    y=seasonal_pattern.values,
                    name="Padr√£o Sazonal",
                    marker_color='lightblue',
                    showlegend=False
                ),
                row=3, col=1
            )
        
        # Layout e configura√ß√µes
        fig.update_layout(
            height=600,
            hovermode="x unified",
            showlegend=True,
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
        )
        
        # T√≠tulos dos eixos
        fig.update_xaxes(title_text="Per√≠odo", row=3, col=1)
        fig.update_yaxes(title_text="Quantidade", row=1, col=1)
        fig.update_yaxes(title_text="Varia√ß√£o (%)", row=2, col=1)
        fig.update_yaxes(title_text="M√©dia Sazonal", row=3, col=1)
        
        st.plotly_chart(fig, use_container_width=True)
        
        # M√©tricas de tend√™ncia
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            trend_direction = "‚ÜóÔ∏è Crescente" if slope > 0 else "‚ÜòÔ∏è Decrescente" if slope < 0 else "‚Üí Est√°vel"
            st.metric("Tend√™ncia", trend_direction)
        with col2:
            st.metric("Correla√ß√£o", f"{r_value:.3f}")
        with col3:
            volatility = pct_change.std()
            st.metric("Volatilidade", f"{volatility:.1f}%")
        with col4:
            last_change = pct_change.iloc[-1] if not pct_change.empty else 0
            st.metric("√öltima Varia√ß√£o", f"{last_change:.1f}%")
    
    # An√°lises de tend√™ncia por dataset
    trend_col1, trend_col2 = st.columns(2)
    
    with trend_col1:
        create_advanced_time_series(df_atend_todos, "Atendimentos", "#3b82f6")
    
    with trend_col2:
        create_advanced_time_series(df_laudos_todos, "Laudos", "#10b981")
    
    # An√°lise de correla√ß√£o cruzada
    if df_atend_todos is not None and df_laudos_todos is not None:
        st.markdown("#### üîó An√°lise de Correla√ß√£o Cruzada")
        
        atend_monthly = df_atend_todos.groupby("anom√™s_dt")["quantidade"].sum()
        laudos_monthly = df_laudos_todos.groupby("anom√™s_dt")["quantidade"].sum()
        common_periods = atend_monthly.index.intersection(laudos_monthly.index)
        
        if len(common_periods) > 3:
            correlation_data = pd.DataFrame({
                "Atendimentos": atend_monthly.loc[common_periods],
                "Laudos": laudos_monthly.loc[common_periods]
            }).reset_index()
            
            correlation_coef = correlation_data["Atendimentos"].corr(correlation_data["Laudos"])
            
            fig_correlation = px.scatter(
                correlation_data,
                x="Atendimentos", 
                y="Laudos",
                trendline="ols",
                title=f"Correla√ß√£o: Atendimentos vs Laudos (r = {correlation_coef:.3f})",
                hover_data=["anom√™s_dt"]
            )
            
            fig_correlation.update_layout(height=400)
            st.plotly_chart(fig_correlation, use_container_width=True)
            
            # Interpreta√ß√£o da correla√ß√£o
            if correlation_coef > 0.8:
                st.success(f"üü¢ **Correla√ß√£o Forte** ({correlation_coef:.3f}): Atendimentos e laudos est√£o bem alinhados")
            elif correlation_coef > 0.5:
                st.warning(f"üü° **Correla√ß√£o Moderada** ({correlation_coef:.3f}): Algum desalinhamento entre atendimentos e laudos")
            else:
                st.error(f"üî¥ **Correla√ß√£o Fraca** ({correlation_coef:.3f}): Atendimentos e laudos n√£o est√£o alinhados")

# ============ ABA 3: RANKINGS ============
with tab3:
    st.subheader("üèÜ Rankings e An√°lises Comparativas")
    
    def create_comprehensive_ranking(df: pd.DataFrame, dimension: str, title: str, top_n: int = 20):
        """Cria ranking abrangente com m√∫ltiplas m√©tricas"""
        if df is None or df.empty or dimension not in df.columns:
            st.info(f"Dados insuficientes para {title}")
            return
        
        # Agrega√ß√£o de dados
        ranking_data = df.groupby(dimension).agg({
            "quantidade": ["sum", "count", "mean", "std"]
        }).round(2)
        
        ranking_data.columns = ["Total", "Registros", "M√©dia", "Desvio"]
        ranking_data = ranking_data.fillna(0)
        
        # C√°lculos adicionais
        ranking_data["Coef_Variacao"] = (ranking_data["Desvio"] / ranking_data["M√©dia"]).replace([np.inf, -np.inf], 0)
        ranking_data["Percentual"] = (ranking_data["Total"] / ranking_data["Total"].sum()) * 100
        ranking_data["Percentual_Acum"] = ranking_data.sort_values("Total", ascending=False)["Percentual"].cumsum()
        
        # Top N
        top_ranking = ranking_data.sort_values("Total", ascending=False).head(top_n).reset_index()
        
        if top_ranking.empty:
            st.info(f"Sem dados para {title}")
            return
        
        # Gr√°fico principal
        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=("Ranking por Volume", "Distribui√ß√£o Percentual"),
            specs=[[{"type": "bar"}, {"type": "pie"}]],
            horizontal_spacing=0.1
        )
        
        # Gr√°fico de barras
        fig.add_trace(
            go.Bar(
                y=top_ranking[dimension],
                x=top_ranking["Total"],
                orientation="h",
                name="Total",
                marker=dict(
                    color=top_ranking["Total"],
                    colorscale="Viridis",
                    showscale=True
                ),
                text=top_ranking["Total"],
                textposition="outside"
            ),
            row=1, col=1
        )
        
        # Gr√°fico de pizza (top 10)
        fig.add_trace(
            go.Pie(
                labels=top_ranking[dimension].head(10),
                values=top_ranking["Percentual"].head(10),
                name="Distribui√ß√£o"
            ),
            row=1, col=2
        )
        
        fig.update_layout(
            height=500,
            showlegend=False,
            title_text=title
        )
        
        fig.update_yaxes(categoryorder="total ascending", row=1, col=1)
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Tabela detalhada
        with st.expander(f"üìä Dados Detalhados - {title}", expanded=False):
            # Formata√ß√£o da tabela
            display_df = top_ranking.copy()
            display_df["Total"] = display_df["Total"].apply(lambda x: format_number(x))
            display_df["M√©dia"] = display_df["M√©dia"].apply(lambda x: format_number(x, 1))
            display_df["Percentual"] = display_df["Percentual"].apply(lambda x: f"{x:.1f}%")
            display_df["Coef_Variacao"] = display_df["Coef_Variacao"].apply(lambda x: f"{x:.2f}")
            
            st.dataframe(display_df, use_container_width=True)
    
    # Tabs de rankings
    rank_tab1, rank_tab2, rank_tab3, rank_tab4 = st.tabs([
        "üè¢ Por Diretoria", 
        "üè™ Por Unidade", 
        "üî¨ Por Tipo", 
        "üìä An√°lise Comparativa"
    ])
    
    with rank_tab1:
        col1, col2 = st.columns(2)
        with col1:
            create_comprehensive_ranking(df_atend_todos, "diretoria", "Atendimentos por Diretoria")
        with col2:
            create_comprehensive_ranking(df_laudos_todos, "diretoria", "Laudos por Diretoria")
    
    with rank_tab2:
        col1, col2 = st.columns(2)
        with col1:
            create_comprehensive_ranking(df_atend_todos, "unidade", "Atendimentos por Unidade", 25)
        with col2:
            create_comprehensive_ranking(df_laudos_todos, "unidade", "Laudos por Unidade", 25)
    
    with rank_tab3:
        col1, col2 = st.columns(2)
        with col1:
            create_comprehensive_ranking(df_atend_esp, "tipo", "Atendimentos por Tipo", 20)
        with col2:
            create_comprehensive_ranking(df_laudos_esp, "tipo", "Laudos por Tipo", 20)
    
    with rank_tab4:
        st.markdown("#### üìä Matriz de Efici√™ncia: Atendimentos vs Laudos")
        
        if df_atend_todos is not None and df_laudos_todos is not None:
            # An√°lise por unidade
            if "unidade" in df_atend_todos.columns and "unidade" in df_laudos_todos.columns:
                atend_unidade = df_atend_todos.groupby("unidade")["quantidade"].sum()
                laudos_unidade = df_laudos_todos.groupby("unidade")["quantidade"].sum()
                
                efficiency_data = pd.DataFrame({
                    "Atendimentos": atend_unidade,
                    "Laudos": laudos_unidade
                }).fillna(0)
                
                efficiency_data["Taxa_Conversao"] = (
                    efficiency_data["Laudos"] / efficiency_data["Atendimentos"] * 100
                ).replace([np.inf, -np.inf], 0)
                
                efficiency_data["Eficiencia_Score"] = (
                    efficiency_data["Taxa_Conversao"] * 0.7 + 
                    (efficiency_data["Laudos"] / efficiency_data["Laudos"].max()) * 30
                )
                
                # Classifica√ß√£o em quadrantes
                mediana_atend = efficiency_data["Atendimentos"].median()
                mediana_laudos = efficiency_data["Laudos"].median()
                
                def classify_quadrant(row):
                    if row["Atendimentos"] >= mediana_atend and row["Laudos"] >= mediana_laudos:
                        return "‚≠ê Alto Volume/Alta Produ√ß√£o"
                    elif row["Atendimentos"] >= mediana_atend and row["Laudos"] < mediana_laudos:
                        return "üîÑ Alto Volume/Baixa Produ√ß√£o"
                    elif row["Atendimentos"] < mediana_atend and row["Laudos"] >= mediana_laudos:
                        return "üéØ Baixo Volume/Alta Efici√™ncia"
                    else:
                        return "üìâ Baixo Volume/Baixa Produ√ß√£o"
                
                efficiency_data["Quadrante"] = efficiency_data.apply(classify_quadrant, axis=1)
                
                # Gr√°fico de dispers√£o
                fig_efficiency = px.scatter(
                    efficiency_data.reset_index(),
                    x="Atendimentos",
                    y="Laudos",
                    size="Taxa_Conversao",
                    color="Quadrante",
                    hover_name="unidade",
                    title="Matriz de Efici√™ncia por Unidade",
                    size_max=20
                )
                
                # Linhas de refer√™ncia
                fig_efficiency.add_vline(
                    x=mediana_atend, 
                    line_dash="dash", 
                    line_color="gray",
                    annotation_text="Mediana Atendimentos"
                )
                fig_efficiency.add_hline(
                    y=mediana_laudos, 
                    line_dash="dash", 
                    line_color="gray",
                    annotation_text="Mediana Laudos"
                )
                
                fig_efficiency.update_layout(height=500)
                st.plotly_chart(fig_efficiency, use_container_width=True)
                
                # Top performers
                st.markdown("**üèÜ Top 10 Unidades Mais Eficientes:**")
                top_efficient = efficiency_data.sort_values("Eficiencia_Score", ascending=False).head(10)
                top_efficient_display = top_efficient.reset_index()
                top_efficient_display["Taxa_Conversao"] = top_efficient_display["Taxa_Conversao"].apply(lambda x: f"{x:.1f}%")
                top_efficient_display["Eficiencia_Score"] = top_efficient_display["Eficiencia_Score"].apply(lambda x: f"{x:.1f}")
                
                st.dataframe(
                    top_efficient_display[["unidade", "Atendimentos", "Laudos", "Taxa_Conversao", "Quadrante"]],
                    use_container_width=True
                )

# ============ ABA 4: PEND√äNCIAS ============
with tab4:
    st.subheader("‚è∞ Gest√£o Avan√ßada de Pend√™ncias")
    
    def analyze_aging_comprehensive(df: pd.DataFrame, title: str, date_column: str = "data_base"):
        """An√°lise abrangente de aging"""
        if df is None or df.empty:
            st.info(f"Sem dados de {title}")
            return
        
        # Buscar coluna de data dispon√≠vel
        date_cols = [col for col in df.columns if "data" in col.lower()]
        if date_column not in df.columns and date_cols:
            date_column = date_cols[0]
        
        if date_column not in df.columns:
            st.warning(f"Coluna de data n√£o encontrada para {title}")
            return
        
        # Processamento de aging
        dates = pd.to_datetime(df[date_column], errors="coerce")
        if dates.isna().all():
            st.warning(f"Datas inv√°lidas em {title}")
            return
        
        hoje = pd.Timestamp.now().normalize()
        aging_days = (hoje - dates).dt.days
        
        # Classifica√ß√£o de aging
        aging_ranges = [
            (0, 15, "0-15 dias", "üü¢"),
            (16, 30, "16-30 dias", "üü°"),
            (31, 60, "31-60 dias", "üü†"),
            (61, 90, "61-90 dias", "üî¥"),
            (91, 180, "91-180 dias", "üî¥"),
            (181, 365, "181-365 dias", "‚ö´"),
            (366, float('inf'), "> 365 dias", "‚ö´")
        ]
        
        def classify_aging(days):
            for min_days, max_days, label, color in aging_ranges:
                if min_days <= days <= max_days:
                    return label, color
            return "Indefinido", "‚ö™"
        
        aging_classifications = aging_days.apply(lambda x: classify_aging(x) if pd.notna(x) else ("Indefinido", "‚ö™"))
        df_analysis = df.copy()
        df_analysis["dias_pendentes"] = aging_days
        df_analysis["faixa_aging"] = [item[0] for item in aging_classifications]
        df_analysis["cor_aging"] = [item[1] for item in aging_classifications]
        
        # Estat√≠sticas
        stats = {
            "total": len(df_analysis),
            "media_dias": float(aging_days.mean()),
            "mediana_dias": float(aging_days.median()),
            "max_dias": int(aging_days.max()) if not aging_days.empty else 0,
            "p90_dias": float(aging_days.quantile(0.9)),
            "criticos": int((aging_days > 90).sum()),
            "urgentes": int((aging_days > 60).sum())
        }
        
        # Layout em colunas
        col1, col2 = st.columns([0.6, 0.4])
        
        with col1:
            st.markdown(f"#### üìä {title} - Distribui√ß√£o de Aging")
            
            # Distribui√ß√£o por faixa
            aging_dist = df_analysis["faixa_aging"].value_counts()
            aging_dist = aging_dist.reindex([label for _, _, label, _ in aging_ranges if label in aging_dist.index])
            
            fig_aging = px.bar(
                x=aging_dist.index,
                y=aging_dist.values,
                title=f"Distribui√ß√£o de {title}",
                color=aging_dist.values,
                color_continuous_scale="Reds"
            )
            
            fig_aging.update_layout(
                height=400,
                showlegend=False,
                xaxis_title="Faixa de Aging",
                yaxis_title="Quantidade"
            )
            
            st.plotly_chart(fig_aging, use_container_width=True)
        
        with col2:
            st.markdown(f"#### üìà Estat√≠sticas de {title}")
            
            # Cards de estat√≠sticas
            stat_col1, stat_col2 = st.columns(2)
            with stat_col1:
                st.metric("Total", format_number(stats["total"]))
                st.metric("Cr√≠ticos (>90d)", format_number(stats["criticos"]))
                st.metric("M√©dia (dias)", format_number(stats["media_dias"], 1))
            
            with stat_col2:
                st.metric("M√°ximo (dias)", format_number(stats["max_dias"]))
                st.metric("P90 (dias)", format_number(stats["p90_dias"], 1))
                st.metric("Mediana (dias)", format_number(stats["mediana_dias"], 1))
            
            # Gr√°fico de pizza - prioridades
            prioridade_map = {
                "Normal": aging_days <= 30,
                "Aten√ß√£o": (aging_days > 30) & (aging_days <= 60),
                "Urgente": (aging_days > 60) & (aging_days <= 90),
                "Cr√≠tico": aging_days > 90
            }
            
            prioridade_counts = {k: v.sum() for k, v in prioridade_map.items()}
            
            fig_priority = px.pie(
                values=list(prioridade_counts.values()),
                names=list(prioridade_counts.keys()),
                title="Distribui√ß√£o por Prioridade",
                color_discrete_map={
                    "Normal": "green",
                    "Aten√ß√£o": "yellow", 
                    "Urgente": "orange",
                    "Cr√≠tico": "red"
                }
            )
            
            fig_priority.update_layout(height=300)
            st.plotly_chart(fig_priority, use_container_width=True)
        
        # An√°lise por dimens√µes
        if "unidade" in df_analysis.columns:
            st.markdown(f"#### üè¢ {title} por Unidade")
            
            unidade_aging = df_analysis.groupby("unidade").agg({
                "dias_pendentes": ["count", "mean", "max"],
                "faixa_aging": lambda x: (x.isin(["61-90 dias", "91-180 dias", "181-365 dias", "> 365 dias"])).sum()
            }).round(1)
            
            unidade_aging.columns = ["Total", "M√©dia_Dias", "Max_Dias", "Cr√≠ticos"]
            unidade_aging = unidade_aging.sort_values("Cr√≠ticos", ascending=False).head(15)
            
            fig_unidade = px.bar(
                unidade_aging.reset_index(),
                x="unidade",
                y="Cr√≠ticos",
                title=f"Top 15 Unidades - {title} Cr√≠ticos",
                color="M√©dia_Dias",
                color_continuous_scale="Reds"
            )
            
            fig_unidade.update_layout(height=400, xaxis_tickangle=-45)
            st.plotly_chart(fig_unidade, use_container_width=True)
        
        # Top casos mais antigos
        st.markdown(f"**üî¥ Top 20 {title} Mais Antigos:**")
        oldest_cases = df_analysis.nlargest(20, "dias_pendentes")
        
        display_cols = []
        if "id" in oldest_cases.columns:
            display_cols.append("id")
        if "unidade" in oldest_cases.columns:
            display_cols.append("unidade")
        if "tipo" in oldest_cases.columns:
            display_cols.append("tipo")
        display_cols.extend(["dias_pendentes", "faixa_aging"])
        
        available_cols = [col for col in display_cols if col in oldest_cases.columns]
        if available_cols:
            st.dataframe(oldest_cases[available_cols], use_container_width=True, height=300)
        
        return df_analysis, stats
    
    # An√°lise de laudos pendentes
    laudos_analysis, laudos_stats = analyze_aging_comprehensive(df_pend_laudos, "Laudos Pendentes")
    
    # An√°lise de exames pendentes  
    exames_analysis, exames_stats = analyze_aging_comprehensive(df_pend_exames, "Exames Pendentes")
    
    # An√°lise comparativa consolidada
    if laudos_stats and exames_stats:
        st.markdown("#### üìä An√°lise Comparativa de Pend√™ncias")
        
        comparison_data = pd.DataFrame({
            "Tipo": ["Laudos", "Exames"],
            "Total": [laudos_stats["total"], exames_stats["total"]],
            "M√©dia_Dias": [laudos_stats["media_dias"], exames_stats["media_dias"]],
            "Cr√≠ticos": [laudos_stats["criticos"], exames_stats["criticos"]],
            "P90_Dias": [laudos_stats["p90_dias"], exames_stats["p90_dias"]]
        })
        
        fig_comparison = px.bar(
            comparison_data,
            x="Tipo",
            y=["Total", "Cr√≠ticos"],
            title="Comparativo: Laudos vs Exames Pendentes",
            barmode="group"
        )
        
        fig_comparison.update_layout(height=400)
        st.plotly_chart(fig_comparison, use_container_width=True)
        
        # Tabela comparativa
        st.dataframe(comparison_data, use_container_width=True)

# ============ ABA 5: AN√ÅLISE DI√ÅRIA ============
with tab5:
    st.subheader("üìÖ An√°lise Operacional Di√°ria")
    
    def process_daily_data(df_atend: pd.DataFrame, df_laudos: pd.DataFrame):
        """Processamento de dados di√°rios"""
        def extract_daily_counts(df: pd.DataFrame, label: str) -> pd.DataFrame:
            if df is None or df.empty or "dia" not in df.columns:
                return pd.DataFrame(columns=["dia", label])
            
            daily_data = (
                df.dropna(subset=["dia"])
                .groupby("dia")["quantidade"]
                .sum()
                .reset_index()
                .rename(columns={"quantidade": label})
                .sort_values("dia")
            )
            return daily_data
        
        atend_daily = extract_daily_counts(df_atend, "Atendimentos")
        laudos_daily = extract_daily_counts(df_laudos, "Laudos")
        
        if atend_daily.empty and laudos_daily.empty:
            return None
        
        # Merge dos dados
        daily_combined = pd.merge(atend_daily, laudos_daily, on="dia", how="outer").fillna(0)
        daily_combined["Atendimentos"] = pd.to_numeric(daily_combined["Atendimentos"], errors="coerce").fillna(0)
        daily_combined["Laudos"] = pd.to_numeric(daily_combined["Laudos"], errors="coerce").fillna(0)
        daily_combined = daily_combined.sort_values("dia")
        
        # C√°lculos adicionais
        daily_combined["Taxa_Conversao"] = np.where(
            daily_combined["Atendimentos"] > 0,
            (daily_combined["Laudos"] / daily_combined["Atendimentos"]) * 100,
            0
        )
        
        # M√©dias m√≥veis
        for period in [7, 14, 30]:
            daily_combined[f"MA{period}_Atendimentos"] = daily_combined["Atendimentos"].rolling(period).mean()
            daily_combined[f"MA{period}_Laudos"] = daily_combined["Laudos"].rolling(period).mean()
            daily_combined[f"MA{period}_Taxa"] = daily_combined["Taxa_Conversao"].rolling(period).mean()
        
        # Detec√ß√£o de tend√™ncias
        if len(daily_combined) >= 30:
            recent_30 = daily_combined.tail(30)
            trend_atend = np.polyfit(range(30), recent_30["Atendimentos"], 1)[0]
            trend_laudos = np.polyfit(range(30), recent_30["Laudos"], 1)[0]
            
            daily_combined["Trend_Atendimentos"] = trend_atend
            daily_combined["Trend_Laudos"] = trend_laudos
        
        # An√°lise de sazonalidade semanal
        daily_combined["Dia_Semana"] = pd.to_datetime(daily_combined["dia"]).dt.day_name()
        daily_combined["Numero_Semana"] = pd.to_datetime(daily_combined["dia"]).dt.isocalendar().week
        
        return daily_combined
    
    daily_data = process_daily_data(df_atend_diario, df_laudos_diario)
    
    if daily_data is None or daily_data.empty:
        st.info("üìù Sem dados di√°rios dispon√≠veis. Carregue os arquivos 'Atendimentos (Di√°rio)' e 'Laudos (Di√°rio)'")
    else:
        # M√©tricas principais
        ultima_data = daily_data["dia"].max()
        ultimo_registro = daily_data[daily_data["dia"] == ultima_data].iloc[0]
        
        col_metrics = st.columns(5)
        with col_metrics[0]:
            st.metric("√öltimo Dia", ultima_data.strftime("%d/%m/%Y"))
        with col_metrics[1]:
            st.metric("Atendimentos", f"{int(ultimo_registro['Atendimentos']):,}".replace(",", "."))
        with col_metrics[2]:
            st.metric("Laudos", f"{int(ultimo_registro['Laudos']):,}".replace(",", "."))
        with col_metrics[3]:
            taxa_ultima = ultimo_registro["Taxa_Conversao"]
            st.metric("Taxa Convers√£o", f"{taxa_ultima:.1f}%")
        with col_metrics[4]:
            dias_analisados = len(daily_data)
            st.metric("Dias Analisados", f"{dias_analisados:,}".replace(",", "."))
        
        # Gr√°fico principal - S√©rie temporal
        st.markdown("#### üìà Evolu√ß√£o Di√°ria Completa")
        
        fig_daily = go.Figure()
        
        # S√©rie principal
        fig_daily.add_trace(go.Scatter(
            x=daily_data["dia"],
            y=daily_data["Atendimentos"],
            mode="lines",
            name="Atendimentos",
            line=dict(color="#3b82f6", width=2)
        ))
        
        fig_daily.add_trace(go.Scatter(
            x=daily_data["dia"],
            y=daily_data["Laudos"],
            mode="lines",
            name="Laudos",
            line=dict(color="#10b981", width=2)
        ))
        
        # M√©dias m√≥veis
        if "MA7_Atendimentos" in daily_data.columns:
            fig_daily.add_trace(go.Scatter(
                x=daily_data["dia"],
                y=daily_data["MA7_Atendimentos"],
                mode="lines",
                name="MM7 Atendimentos",
                line=dict(color="#3b82f6", width=2, dash="dash"),
                opacity=0.7
            ))
            
            fig_daily.add_trace(go.Scatter(
                x=daily_data["dia"],
                y=daily_data["MA7_Laudos"],
                mode="lines",
                name="MM7 Laudos",
                line=dict(color="#10b981", width=2, dash="dash"),
                opacity=0.7
            ))
        
        fig_daily.update_layout(
            height=chart_height,
            hovermode="x unified",
            xaxis_title="Data",
            yaxis_title="Quantidade",
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
        )
        
        st.plotly_chart(fig_daily
